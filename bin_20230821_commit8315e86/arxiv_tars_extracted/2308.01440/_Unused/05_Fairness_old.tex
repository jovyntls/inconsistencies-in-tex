\section{Fairness in Cellular Network Configuration}\label{SM-MP-Chapter}

The performance function $\Phi_{\mathtt{SINR}}$ studied in the previous section takes into account the SINR provided in dB and averaged over all network users. While optimizing this performance function improves the average SINR in dB, it can lead to significant performance disparities among individual network users. To achieve a desired balance, it is important to introduce appropriate SINR performance measures that provide a level of fairness among different users. Two such measures, namely the Max-Product and Soft Max-Min proxy performance functions \cite{nikbakht2020unsupervised}, are studied in Sections \ref{MP-Subsection} and \ref{SM-Subsection}, respectively. These proxy performance functions aim to promote fairness and balance the users' SINR at the expense of a reduced average performance.




\subsection{Max-Product Proxy Performance Function}\label{MP-Subsection}


\subsubsection{Performance Function}\label{MP-Objective}
To avoid low SINR values, the goal of the max-product performance function is to maximize the product of SINRs. Obviously, the outcome of such an optimization cannot result in extremely small SINR values since such a case will create low SINR product \cite{nikbakht2020unsupervised}. 
%The main property of the max-product performance function is to provide a trade-off between the average performance and fairness. The goal is to maximize the product of SINRs while discouraging users with extremely low or high SINR values. 
The max-product proxy performance function is defined as:
\begin{align}\label{max-product-objective}
    \Phi_{{\sf MP}}(\bm{V}, \mathbf{\Theta}, \bm{\rho}) &=  \sum_{n=1}^{N} \int_{V_n} \gamma^{(n)}_{{\sf MP}} (\bm{q}; \mathbf{\Theta}, \bm{\rho}) \lambda(\bm{q}) d\bm{q}, \\
    \textrm{s.t. } \rho_n &\leq \rho_{\max} \qquad \forall n \in \{1,\cdots, N\}, \label{max-product-objective-constraint}
\end{align}
where
\begin{align}\label{Max-product}
\gamma_{\sf MP}^{(n)}(\bm{q}; \mathbf{\Theta}, \bm{\rho}) = - \log \! \left[ \mu + \frac{1}{(\mathtt{SINR^{(n)}_{lin}}(\bm{q}; \mathbf{\Theta}, \bm{\rho}) + \nu )} \right].
\end{align}
The offset $\nu$ added to $\mathtt{SINR_{lin}}$ avoids the performance domination by users with very low SINRs. The second offset $\mu$ plays a similar role for high SINRs, e.g., $\mu=0.01$ stops rewarding SINRs above $20$dB. Note that for the special case of $\mu = \nu = 0$, the max-product proxy performance function \eqref{max-product-objective} is the same as  $\Phi_{\mathtt{SINR}}$ in \eqref{SINR-objective} times a constant multiplier. As a result, for $\mu = \nu = 0$, maximizing  \eqref{max-product-objective} or \eqref{SINR-objective} results in the same solution and for a non-zero $\mu$ and/or $\nu$ value, \eqref{max-product-objective} can be considered as a generalization of \eqref{SINR-objective}.
%For the specific scenario where $\mu = \nu = 0$, it is important to highlight that the performance function of the max-product proxy, as shown in Eq. (\ref{max-product-objective}), becomes equivalent to that of $\Phi_{\mathtt{SINR}}$ stated in Eq. (\ref{SINR-objective}), except for a constant multiplier.




\subsubsection{Optimal Configuration}\label{MP-Optimal-Configuration}

The iterative process for maximizing the constrained performance function described in Eqs. (\ref{max-product-objective}) and (\ref{max-product-objective-constraint}) over variables $\bm{V}$, $\bm{\Theta}$, and $\bm{\rho}$ is similar to the one outlined in Section \ref{SM-Subsection}. This process requires determining the optimality conditions for each variable while keeping the other two variables constant.

\begin{Proposition}\label{MP-optimal-cell}
For a given BS vertical antenna tilts $\bm{\Theta}$ and transmission powers $\bm{\rho}$, the optimal cell partitioning $\bm{V}^*(\bm{\Theta}, \bm{\rho}) = \left(V^*_1(\bm{\Theta}, \bm{\rho}), \cdots, V_N^*(\bm{\Theta}, \bm{\rho})\right)$ that maximizes $\Phi_{{\sf MP}}$ is given by:
\begin{multline}\label{optimal-cell-partitioning-MP}
    \!\!\!\!\! V_n^*(\bm{\Theta}, \bm{\rho}) = \big\{\bm{q} \in Q \mid \mathtt{RSS_{dBm}^{(n)}}(\bm{q}; \theta_n, \rho_n) \geq \mathtt{RSS_{dBm}^{(k)}}(\bm{q}; \theta_k, \rho_k), \\ \textrm{ for all } 1 \leq k \leq N \big\}.
\end{multline}        
\end{Proposition}
\textit{Proof. }
Since $\mu$ and $\nu$ are constants, $\gamma_{\sf MP}^{(n)}(\bm{q}; \mathbf{\Theta}, \bm{\rho}) \geq \gamma_{\sf MP}^{(k)}(\bm{q}; \mathbf{\Theta}, \bm{\rho})$ for all $k \neq n$ is the same as Eq. \eqref{optimal-cell-partitioning-SINR-proof} and the rest of the proof follows from that of Proposition \ref{optimal-V-SINR}. $\hfill\blacksquare$ 
%In order to achieve an optimal association between the network user located at position $\bm{q} \in Q$ and BS $n$, $\gamma_{\sf MP}^{(n)}(\bm{q}; \mathbf{\Theta}, \bm{\rho})$ must be no less than $\gamma_{\sf MP}^{(k)}(\bm{q}; \mathbf{\Theta}, \bm{\rho})$ for any other index $k \neq n$. The inequality $\gamma_{\sf MP}^{(n)}(\bm{q}; \mathbf{\Theta}, \bm{\rho}) \geq \gamma_{\sf MP}^{(k)}(\bm{q}; \mathbf{\Theta}, \bm{\rho})$ is equivalent to base station $n$ providing the maximum SINR value at the user location $\bm{q}$, which is mathematically asserted in Eq. (\ref{optimal-cell-partitioning-SINR-proof}). Thus, the remainder of the proof follows the same reasoning as in the proof of Proposition \ref{optimal-V-SINR}. $\hfill\blacksquare$


Next, we provide the partial derivative expression for $\Phi_{{\sf MP}}$ w.r.t. the BS $n$'s antenna tilt $\theta_n$.


\begin{Proposition}\label{gradient-theta-MP}
    The partial derivative of the performance function $\Phi_{{\sf MP}}$ w.r.t. the BS $n$'s vertical antenna tilt $\theta_n$ is given by Eq. (\ref{gradient-theta-MP-equation}),
    % \begin{figure*}[t!]
% \begin{multline}\label{gradient-theta-MP-equation}
%     \frac{\partial \Phi_{{\sf MP}}}{\partial \theta_n} = \Bigg\{ \sum_{u=1}^{N_U} \int_{V_n \cap Q_u}  \frac{ \SINR^{(n)}_{\mathtt{lin}} \cdot \frac{2.4 \log 10}{\theta^2_{\textrm{3dB}}} \cdot \left(\theta_{n,\bm{q}} - \theta_n \right)}{\left[\SINR^{(n)}_{\mathtt{lin}} + \nu \right] \cdot \left[ 1 + \mu \left( \SINR^{(n)}_{\mathtt{lin}} + \nu \right) \right] }  \lambda(\bm{q})d\bm{q} \\ +  \int_{V_n \cap Q_G}  \frac{ \SINR^{(n)}_{\mathtt{lin}} \cdot \frac{2.4 \log 10}{\theta^2_{\textrm{3dB}}} \cdot \left(\theta_{n,\bm{q}} - \theta_n \right)}{\left[\SINR^{(n)}_{\mathtt{lin}} + \nu \right] \cdot \left[ 1 + \mu \left( \SINR^{(n)}_{\mathtt{lin}} + \nu \right) \right] }    \lambda(\bm{q})d\bm{q} \Bigg\} \\
%     - \sum_{i\neq n} \Bigg\{ \sum_{u=1}^{N_U} \int_{V_i \cap Q_u}  \frac{\SINR^{(i)}_{\mathtt{lin}} \cdot \frac{2.4 \log 10}{\theta^2_{\text{3dB}}} \cdot \left(\theta_{n,q} - \theta_n \right)\cdot \RSS^{(n)}_{\mathtt{lin}}  }{\left[\SINR^{(i)}_{\mathtt{lin}} + \nu \right] \cdot 
%     \left[ 1 + \mu \left( \SINR^{(i)}_{\mathtt{lin}} + \nu \right) \right] \cdot \left[ {\sum_{j\neq i}^{} \RSS^{(j)}_{\mathtt{lin}}  + \sigma^2_{\mathtt{lin}}} \right] }   \lambda(\bm{q})d\bm{q} \\ + \int_{V_i \cap Q_G}   \frac{\SINR^{(i)}_{\mathtt{lin}} \cdot \frac{2.4 \log 10}{\theta^2_{\text{3dB}}} \cdot \left(\theta_{n,q} - \theta_n \right)\cdot \RSS^{(n)}_{\mathtt{lin}}  }{\left[\SINR^{(i)}_{\mathtt{lin}} + \nu \right] \cdot 
%     \left[ 1 + \mu \left( \SINR^{(i)}_{\mathtt{lin}} + \nu \right) \right] \cdot \left[ {\sum_{j\neq i}^{} \RSS^{(j)}_{\mathtt{lin}}  + \sigma^2_{\mathtt{lin}}} \right] }   \lambda(\bm{q})d\bm{q}  \Bigg \},    
% \end{multline}   
\begin{figure*}[t!]
\begin{multline}\label{gradient-theta-MP-equation}
    \frac{\partial \Phi_{{\sf MP}}}{\partial \theta_n} =   \int_{V_n}  \frac{ \SINR^{(n)}_{\mathtt{lin}} \cdot \frac{2.4 \log 10}{\theta^2_{\textrm{3dB}}} \cdot \left(\theta_{n,\bm{q}} - \theta_n \right)}{\left[\SINR^{(n)}_{\mathtt{lin}} + \nu \right] \cdot \left[ 1 + \mu \left( \SINR^{(n)}_{\mathtt{lin}} + \nu \right) \right] }  \lambda(\bm{q})d\bm{q} \\
    - \sum_{i\neq n}   \int_{V_i }  \frac{\SINR^{(i)}_{\mathtt{lin}} \cdot \frac{2.4 \log 10}{\theta^2_{\text{3dB}}} \cdot \left(\theta_{n,q} - \theta_n \right)\cdot \RSS^{(n)}_{\mathtt{lin}}  }{\left[\SINR^{(i)}_{\mathtt{lin}} + \nu \right] \cdot 
    \left[ 1 + \mu \left( \SINR^{(i)}_{\mathtt{lin}} + \nu \right) \right] \cdot \left[ {\sum_{j\neq i}^{} \RSS^{(j)}_{\mathtt{lin}}  + \sigma^2_{\mathtt{lin}}} \right] }   \lambda(\bm{q})d\bm{q}   ,  
\end{multline}   
\end{figure*}
where for the sake of brevity of the notation, the dependence of the variables $\mathtt{SINR}_{\mathtt{lin}}^{(n)}$, $\mathtt{RSS}_{\mathtt{lin}}^{(n)}$, and $V_n$ on $\bm{\Theta}$ and $\bm{\rho}$ is omitted.
\end{Proposition}
The proof of Proposition \ref{gradient-theta-MP} is provided in Appendix \ref{Appendix_C}.



Next, we provide the partial derivative of $\Phi_{{\sf MP}}$ w.r.t. $\rho_n$.
%Finally, the partial derivative of $\Phi_{{\sf MP}}$ w.r.t. $\rho_n$ is given by the following equation:

\begin{Proposition}\label{gradient-formula-MP-power}
The partial derivative of $\Phi_{{\sf MP}}$ w.r.t. the BS transmission power $\rho_n$ is given by Eq. (\ref{gradient-MP-power}),
% \begin{figure*}[t!]
%     \begin{multline}\label{gradient-MP-power}
%     \frac{\partial \Phi_{{\sf MP}}}{\partial \rho_n} = \Bigg\{ \sum_{u=1}^{N_U} \int_{V_n \cap Q_u}  \frac{\SINR^{(n)}_{\mathtt{lin}} \cdot \frac{\log 10}{10}}{\left[\SINR^{(n)}_{\mathtt{lin}} + \nu \right] \cdot 
%     \left[ 1 + \mu \left( \SINR^{(n)}_{\mathtt{lin}} + \nu \right) \right] }   \lambda(\bm{q})d\bm{q} \\ + \int_{V_n \cap Q_G}   \frac{\SINR^{(n)}_{\mathtt{lin}} \cdot \frac{\log 10}{10}}{\left[\SINR^{(n)}_{\mathtt{lin}} + \nu \right] \cdot 
%     \left[ 1 + \mu \left( \SINR^{(n)}_{\mathtt{lin}} + \nu \right) \right] }   \lambda(\bm{q})d\bm{q} \Bigg\} \\
%     - \sum_{i\neq n} \Bigg\{ \sum_{u=1}^{N_U} \int_{V_i \cap Q_u}     \frac{\left[\frac{1}{10}\log (10) \cdot \left[ \SINR^{(i)}_{\mathtt{lin}} \right]^2 \cdot \frac{\RSS^{(n)}_{\mathtt{lin}}}{\RSS^{(i)}_{\mathtt{lin}}}
%     \right]}{\left[\SINR^{(i)}_{\mathtt{lin}} + \nu \right] \cdot 
%     \left[ 1 + \mu \left( \SINR^{(i)}_{\mathtt{lin}} + \nu \right) \right] }  
%      \lambda(\bm{q})d\bm{q} \\ + \int_{V_i \cap Q_G}     \frac{\left[\frac{1}{10}\log (10) \cdot \left[ \SINR^{(i)}_{\mathtt{lin}} \right]^2 \cdot \frac{\RSS^{(n)}_{\mathtt{lin}}}{\RSS^{(i)}_{\mathtt{lin}}}
%     \right]}{\left[\SINR^{(i)}_{\mathtt{lin}} + \nu \right] \cdot 
%     \left[ 1 + \mu \left( \SINR^{(i)}_{\mathtt{lin}} + \nu \right) \right] }  \lambda(\bm{q})d\bm{q}  \Bigg \},
% \end{multline}
% \end{figure*}
\begin{figure*}[t!]
    \begin{multline}\label{gradient-MP-power}
    \frac{\partial \Phi_{{\sf MP}}}{\partial \rho_n} =   \int_{V_n }  \frac{\SINR^{(n)}_{\mathtt{lin}} \cdot \frac{\log 10}{10}}{\left[\SINR^{(n)}_{\mathtt{lin}} + \nu \right] \cdot 
    \left[ 1 + \mu \left( \SINR^{(n)}_{\mathtt{lin}} + \nu \right) \right] }   \lambda(\bm{q})d\bm{q}   
    - \sum_{i\neq n}   \int_{V_i }     \frac{\left[\frac{1}{10}\log (10) \cdot \left[ \SINR^{(i)}_{\mathtt{lin}} \right]^2 \cdot \frac{\RSS^{(n)}_{\mathtt{lin}}}{\RSS^{(i)}_{\mathtt{lin}}}
    \right]}{\left[\SINR^{(i)}_{\mathtt{lin}} + \nu \right] \cdot 
    \left[ 1 + \mu \left( \SINR^{(i)}_{\mathtt{lin}} + \nu \right) \right] }  
     \lambda(\bm{q})d\bm{q},
\end{multline}
\end{figure*}
where $\SINR^{(n)}_{\mathtt{lin}}(\bm{q}; \bm{\Theta}, \bm{\rho})$, $\RSS^{(n)}_{\mathtt{lin}}(\bm{q}; \theta_n, \rho_n)$, and $V_n(\bm{\Theta}, \bm{\rho})$ are abbreviated as
$\SINR^{(n)}_{\mathtt{lin}} $, $\RSS^{(n)}_{\mathtt{lin}}$, and $V_n$, respectively. 
\end{Proposition}
The proof of Proposition \ref{gradient-formula-MP-power} is provided in Appendix \ref{Appendix_D}.



\subsubsection{Proposed Algorithm}\label{MP-Algorithm}

Using Propositions \ref{MP-optimal-cell}, \ref{gradient-theta-MP}, and \ref{gradient-formula-MP-power}, %we propose an iterative algorithm to maximize the constrained optimization problem in Eqs. (\ref{max-product-objective}) and (\ref{max-product-objective-constraint}) over the cell partitioning, BS vertical antenna tilts, and transmission powers. 
after a random initialization for the values of $\bm{V}$, $\bm{\Theta}$, and $\bm{\rho}$, our %algorithm, referred to as the 
max-product power allocation and vertical antenna tilt (MP-PA-VAT) optimization algorithm, iterates over the following main three steps until its convergence criterion is met: 
\begin{itemize}
    \item Adjust the cell $V_n$ according to Eq. (\ref{optimal-cell-partitioning-MP}) for each $n \in \{1, \cdots, N\}$ while $\bm{\Theta}$ and $\bm{\rho}$ are fixed;
    \item Calculate the gradient $\nabla_{\bm{\Theta}}\Phi_{{\sf MP}}$ according to Proposition \ref{gradient-theta-MP} and apply the gradient ascent algorithm to optimize $\bm{\Theta}$ while $\bm{V}$ and $\bm{\rho}$ are fixed;
    \item Compute the gradient vector $\nabla_{\bm{\rho}}\Phi_{{\sf MP}}$ according to Proposition \ref{gradient-formula-MP-power} and use the projected gradient ascent algorithm to optimize $\bm{\rho}$ within the confined space $(-\infty, \rho_{\max}]^N$ while $\bm{V}$ and $\bm{\Theta}$ are fixed.
\end{itemize}
%The MP-PA-VAT algorithm iterates between the above three steps until the convergence criteria is met, i.e., the relative improvement in the performance function $\Phi_{{\sf MP}}$ becomes less than a predefined threshold.


\begin{Proposition}\label{MP-PA-VAT-convergence}
    The MP-PA-VAT algorithm is an iterative improvement algorithm and converges.
\end{Proposition}

The proof is similar to that of Proposition \ref{BS-PA-VAT-convergence} and is omitted.




\subsection{Soft Max-Min Proxy Performance Function}\label{SM-Subsection}


\subsubsection{Performance Function}\label{SM-Objective}
The primary objective of the soft max-min performance function is to promote fairness for individual user SINR. The soft max-min performance function is formulated as:
\begin{align}\label{soft-max-min-objective}
    \Phi_{{\sf SM}}(\bm{V}, \mathbf{\Theta}, \bm{\rho}) &=  \sum_{n=1}^{N} \int_{V_n} \gamma^{(n)}_{{\sf SM}} (\bm{q}; \mathbf{\Theta}, \bm{\rho}) \lambda(\bm{q}) d\bm{q}, \\
    \textrm{s.t. } \rho_n &\leq \rho_{\max} \qquad \forall n \in \{1,\cdots, N\}, \label{soft-max-min-objective-constraint}
\end{align}
where 
\begin{align}\label{Soft-max-min}
\gamma^{(n)}_{\sf SM}(\bm{q}; \mathbf{\Theta}, \bm{\rho})
= - \exp  \Bigg[ \frac{\alpha}{\big(\mathtt{SINR^{(n)}_{lin}}(\bm{q}; \mathbf{\Theta}, \bm{\rho}) + \nu \big)^{\xi}}\Bigg].
\end{align}

The regulating parameter $\alpha$ controls the softness of the max-min policy. This is because large $\alpha$ values result in the domination of the smallest SINR in $\gamma_{{\sf SM}}$. Therefore, for large $\alpha$ values, the optimization of $\Phi_{{\sf SM}}$ reduces to the maximization of the minimum SINR, i.e., a max-min problem. On the other hand, When $\alpha$ is small, more SINR values will affect the performance function. The small offset parameter $\nu$ is added to avoid having the performance being dominated by users with very low SINR, e.g., $\nu = 0.01$ tends to ignore SINRs below $-20$dB. The exponent $\xi \leq 1$ compresses the dynamic range and enhances high-SINR throughput.  



\subsubsection{Optimal Configuration}\label{SM-Optimal-Configuration}

As before, we derive the necessary optimality conditions for each variable while holding the other two variables fixed to optimize the performance function $\Phi_{{\sf SM}}(\bm{V}, \mathbf{\Theta}, \bm{\rho})$ over cell partitioning, BS antenna tilts, and transmission powers.
\begin{Proposition}\label{optimal-cell-partitioning-SM-performance}
    The optimal cell partitioning $\bm{V}^*(\bm{\Theta}, \bm{\rho}) = \left(V_1^*(\bm{\Theta}, \bm{\rho}), \cdots, V_N^*(\bm{\Theta}, \bm{\rho}) \right)$ that maximizes the performance function $\Phi_{{\sf SM}}$ for a given $\bm{\Theta}$ and $\bm{\rho}$ is given by:
\begin{multline}\label{optimal-cell-partitioning-SM}
   \!\!\!\!\! V_n^*(\bm{\Theta}, \bm{\rho}) = \big\{\bm{q} \in Q \mid \mathtt{RSS_{dBm}^{(n)}}(\bm{q}; \theta_n, \rho_n) \geq \mathtt{RSS_{dBm}^{(k)}}(\bm{q}; \theta_k, \rho_k), \\ \textrm{ for all } 1 \leq k \leq N \big\}.
\end{multline}    
\end{Proposition}
\textit{Proof. }
Since $\alpha$, $\nu$, and $\xi$ are constants, $\gamma_{\sf SM}^{(n)}(\bm{q}; \mathbf{\Theta}, \bm{\rho}) \geq \gamma_{\sf SM}^{(k)}(\bm{q}; \mathbf{\Theta}, \bm{\rho})$ for all $k \neq n$ is the same as Eq. \eqref{optimal-cell-partitioning-SINR-proof} and the rest of the proof follows from that of Proposition \ref{optimal-V-SINR}. $\hfill\blacksquare$ 


% \begin{figure*}[t!]
%     \begin{multline}\label{gradient-vector-theta-SM-equation}
%     \frac{\partial \Phi_{{\sf SM}}}{\partial \theta_n} = \Bigg\{ \sum_{u=1}^{N_U} \int_{V_n \cap Q_u}  \frac{2.4 \log 10 \cdot \alpha \, \xi \,\left(\theta_{n,\bm{q}} - \theta_n \right) \SINR^{(n)}_{\mathtt{lin}} }{\left(\SINR^{(n)}_{\mathtt{lin}} + \nu\right)^{\xi+1} \cdot \theta^2_{\text{3dB}}} 
%     \cdot \exp \! \left[ \frac{\alpha}{(\SINR^{(n)}_{\mathtt{lin}} + \nu )^{\xi}}\right]   \lambda(\bm{q})d\bm{q} \\ + \int_{V_n \cap Q_G}   \frac{2.4 \log 10 \cdot \alpha \, \xi \,\left(\theta_{n,\bm{q}} - \theta_n \right) \SINR^{(n)}_{\mathtt{lin}} }{\left(\SINR^{(n)}_{\mathtt{lin}} + \nu\right)^{\xi+1} \cdot \theta^2_{\text{3dB}}} 
%     \cdot \exp \! \left[ \frac{\alpha}{(\SINR^{(n)}_{\mathtt{lin}} + \nu )^{\xi}}\right]   \lambda(\bm{q})d\bm{q} \Bigg\} \\
%     - \sum_{i\neq n} \Bigg\{ \sum_{u=1}^{N_U} \int_{V_i \cap Q_u}  \frac{\alpha \, \xi \, \SINR^{(i)}_{\mathtt{lin}}}{\left(\SINR^{(i)}_{\mathtt{lin}} + \nu\right)^{\xi+1}} 
%     \cdot \exp \! \left[ \frac{\alpha}{(\SINR^{(i)}_{\mathtt{lin}} + \nu )^{\xi}}\right] \times  \\
%     \left[ \frac{2.4 \log 10}{\theta^2_{\text{3dB}}} \cdot \left(\theta_{n,\bm{q}} - \theta_n \right) \cdot {\frac{\RSS^{(n)}_{\mathtt{lin}}}{\sum_{j\neq i}^{} \RSS^{(j)}_{\mathtt{lin}} + \sigma^2_{\mathtt{lin}}}} \right]   \lambda(\bm{q})d\bm{q} \\ + \int_{V_i \cap Q_G}  \frac{\alpha \, \xi \, \SINR^{(i)}_{\mathtt{lin}}}{\left(\SINR^{(i)}_{\mathtt{lin}} + \nu\right)^{\xi+1}} 
%     \cdot \exp \! \left[ \frac{\alpha}{(\SINR^{(i)}_{\mathtt{lin}} + \nu )^{\xi}}\right] \times  \\
%     \left[ \frac{2.4 \log 10}{\theta^2_{\text{3dB}}} \cdot \left(\theta_{n,\bm{q}} - \theta_n \right) \cdot {\frac{\RSS^{(n)}_{\mathtt{lin}}}{\sum_{j\neq i}^{} \RSS^{(j)}_{\mathtt{lin}} + \sigma^2_{\mathtt{lin}}}} \right]  \lambda(\bm{q})d\bm{q}  \Bigg \},    
% \end{multline}
% \end{figure*}
\begin{figure*}[t!]
    \begin{multline}\label{gradient-vector-theta-SM-equation}
    \frac{\partial \Phi_{{\sf SM}}}{\partial \theta_n} =   \int_{V_n }  \frac{2.4 \log 10 \cdot \alpha \, \xi \,\left(\theta_{n,\bm{q}} - \theta_n \right) \SINR^{(n)}_{\mathtt{lin}} }{\left(\SINR^{(n)}_{\mathtt{lin}} + \nu\right)^{\xi+1} \cdot \theta^2_{\text{3dB}}} 
    \cdot \exp \! \left[ \frac{\alpha}{(\SINR^{(n)}_{\mathtt{lin}} + \nu )^{\xi}}\right]   \lambda(\bm{q})d\bm{q}   \\
    - \sum_{i\neq n}   \int_{V_i }  \frac{\alpha \, \xi \, \SINR^{(i)}_{\mathtt{lin}}}{\left(\SINR^{(i)}_{\mathtt{lin}} + \nu\right)^{\xi+1}} 
    \cdot \exp \! \left[ \frac{\alpha}{(\SINR^{(i)}_{\mathtt{lin}} + \nu )^{\xi}}\right] \times  
    \left[ \frac{2.4 \log 10}{\theta^2_{\text{3dB}}} \cdot \left(\theta_{n,\bm{q}} - \theta_n \right) \cdot {\frac{\RSS^{(n)}_{\mathtt{lin}}}{\sum_{j\neq i}^{} \RSS^{(j)}_{\mathtt{lin}} + \sigma^2_{\mathtt{lin}}}} \right]   \lambda(\bm{q})d\bm{q}   ,    
\end{multline}
\end{figure*}







% \begin{figure*}[t!]
%     \begin{multline}\label{gradient-SM-wrt-power-equation}
%     \frac{\partial \Phi_{{\sf SM}}}{\partial \rho_n} = \Bigg\{ \sum_{u=1}^{N_U} \int_{V_n \cap Q_u} \frac{\alpha\xi \cdot \mathtt{SINR_{\mathtt{lin}}^{(n)}} \cdot \frac{\ln(10)}{10} }{\big(\mathtt{SINR_{\mathtt{lin}}^{(n)}} + \nu\big)^{\xi + 1}} \cdot e^{\frac{\alpha}{\big(\mathtt{SINR^{(n)}_{\mathtt{lin}}} + \nu \big)^\xi}}   \lambda(\bm{q})d\bm{q} \\+ \int_{V_n \cap Q_G} \frac{\alpha\xi \cdot \mathtt{SINR_{\mathtt{lin}}^{(n)}} \cdot \frac{\ln(10)}{10} }{\big(\mathtt{SINR_{\mathtt{lin}}^{(n)}} + \nu\big)^{\xi + 1}} \cdot e^{\frac{\alpha}{\big(\mathtt{SINR^{(n)}_{\mathtt{lin}}} + \nu \big)^\xi}}   \lambda(\bm{q})d\bm{q} \Bigg\} \\
%     - \sum_{i\neq n} \Bigg\{ \sum_{u=1}^{N_U} \int_{V_i \cap Q_u} \frac{\alpha\xi\cdot \big[\mathtt{SINR_{\mathtt{lin}}^{(i)}} \big]^2 \cdot \frac{\ln(10)}{10}\cdot \mathtt{RSS_{\mathtt{lin}}^{(n)}}    }{\big(\mathtt{SINR_{\mathtt{lin}}^{(i)}} + \nu\big)^{\xi + 1}\cdot \mathtt{RSS_{\mathtt{lin}}^{(i)}}} \cdot e^{\frac{\alpha}{\big(\mathtt{SINR_{\mathtt{lin}}^{(i)}} + \nu \big)^\xi}}   \lambda(\bm{q})d\bm{q} \\+ \int_{V_i \cap Q_G} \frac{\alpha\xi\cdot \big[\mathtt{SINR_{\mathtt{lin}}^{(i)}} \big]^2 \cdot \frac{\ln(10)}{10}\cdot \mathtt{RSS_{\mathtt{lin}}^{(n)}}    }{\big(\mathtt{SINR_{\mathtt{lin}}^{(i)}} + \nu\big)^{\xi + 1}\cdot \mathtt{RSS_{\mathtt{lin}}^{(i)}}} \cdot e^{\frac{\alpha}{\big(\mathtt{SINR_{\mathtt{lin}}^{(i)}} + \nu \big)^\xi}}   \lambda(\bm{q})d\bm{q}  \Bigg \},
% \end{multline}
% \end{figure*}
\begin{figure*}[t!]
    \begin{multline}\label{gradient-SM-wrt-power-equation}
    \frac{\partial \Phi_{{\sf SM}}}{\partial \rho_n} =   \int_{V_n } \frac{\alpha\xi \cdot \mathtt{SINR_{\mathtt{lin}}^{(n)}} \cdot \frac{\ln(10)}{10} }{\big(\mathtt{SINR_{\mathtt{lin}}^{(n)}} + \nu\big)^{\xi + 1}} \cdot e^{\frac{\alpha}{\big(\mathtt{SINR^{(n)}_{\mathtt{lin}}} + \nu \big)^\xi}}   \lambda(\bm{q})d\bm{q}   
    - \sum_{i\neq n}   \int_{V_i } \frac{\alpha\xi\cdot \big[\mathtt{SINR_{\mathtt{lin}}^{(i)}} \big]^2 \cdot \frac{\ln(10)}{10}\cdot \mathtt{RSS_{\mathtt{lin}}^{(n)}}    }{\big(\mathtt{SINR_{\mathtt{lin}}^{(i)}} + \nu\big)^{\xi + 1}\cdot \mathtt{RSS_{\mathtt{lin}}^{(i)}}} \cdot e^{\frac{\alpha}{\big(\mathtt{SINR_{\mathtt{lin}}^{(i)}} + \nu \big)^\xi}}   \lambda(\bm{q})d\bm{q}  ,
\end{multline}
\end{figure*}

The following proposition provides the gradient of the performance function w.r.t. the vertical antenna tilts.
%We will then furnish the gradient of the performance function w.r.t. the vertical antenna tilts of the BSs, which can be utilized in a gradient ascent algorithm to enhance the optimization of the BSs' vertical antenna tilts.
\begin{Proposition}\label{gradient-vector-theta-SM}
    The partial derivative of $\Phi_{{\sf SM}}$ w.r.t. the BS vertical antenna tilt $\theta_n$ is given by Eq. (\ref{gradient-vector-theta-SM-equation}),
% \begin{multline}\label{gradient-vector-theta-SM-equation}
%     \frac{\partial \Phi_{{\sf SM}}}{\partial \theta_n} = \Bigg\{ \sum_{u=1}^{N_U} \int_{V_n \cap Q_u}  \frac{2.4 \log 10 \cdot \alpha \, \xi \,\left(\theta_{n,\bm{q}} - \theta_n \right) \SINR^{(n)}_{\mathtt{lin}} }{\left(\SINR^{(n)}_{\mathtt{lin}} + \nu\right)^{\xi+1} \cdot \theta^2_{\text{3dB}}} 
%     \cdot \exp \! \left[ \frac{\alpha}{(\SINR^{(n)}_{\mathtt{lin}} + \nu )^{\xi}}\right]   \lambda(\bm{q})d\bm{q} \\ + \int_{V_n \cap Q_G}   \frac{2.4 \log 10 \cdot \alpha \, \xi \,\left(\theta_{n,\bm{q}} - \theta_n \right) \SINR^{(n)}_{\mathtt{lin}} }{\left(\SINR^{(n)}_{\mathtt{lin}} + \nu\right)^{\xi+1} \cdot \theta^2_{\text{3dB}}} 
%     \cdot \exp \! \left[ \frac{\alpha}{(\SINR^{(n)}_{\mathtt{lin}} + \nu )^{\xi}}\right]   \lambda(\bm{q})d\bm{q} \Bigg\} \\
%     - \sum_{i\neq n} \Bigg\{ \sum_{u=1}^{N_U} \int_{V_i \cap Q_u}  \frac{\alpha \, \xi \, \SINR^{(i)}_{\mathtt{lin}}}{\left(\SINR^{(i)}_{\mathtt{lin}} + \nu\right)^{\xi+1}} 
%     \cdot \exp \! \left[ \frac{\alpha}{(\SINR^{(i)}_{\mathtt{lin}} + \nu )^{\xi}}\right] \times  \\   \left[ \frac{2.4 \log 10}{\theta^2_{\text{3dB}}} \cdot \left(\theta_{n,\bm{q}} - \theta_n \right) \cdot {\frac{\RSS^{(n)}_{\mathtt{lin}}}{\sum_{j\neq i}^{} \RSS^{(j)}_{\mathtt{lin}} + \sigma^2_{\mathtt{lin}}}} \right]   \lambda(\bm{q})d\bm{q} \\ + \int_{V_i \cap Q_G}  \frac{\alpha \, \xi \, \SINR^{(i)}_{\mathtt{lin}}}{\left(\SINR^{(i)}_{\mathtt{lin}} + \nu\right)^{\xi+1}} 
%     \cdot \exp \! \left[ \frac{\alpha}{(\SINR^{(i)}_{\mathtt{lin}} + \nu )^{\xi}}\right] \times  \\   \left[ \frac{2.4 \log 10}{\theta^2_{\text{3dB}}} \cdot \left(\theta_{n,\bm{q}} - \theta_n \right) \cdot {\frac{\RSS^{(n)}_{\mathtt{lin}}}{\sum_{j\neq i}^{} \RSS^{(j)}_{\mathtt{lin}} + \sigma^2_{\mathtt{lin}}}} \right]  \lambda(\bm{q})d\bm{q}  \Bigg \},    
% \end{multline}
where $\SINR^{(n)}_{\mathtt{lin}}(\bm{q}; \bm{\Theta}, \bm{\rho})$, $\RSS^{(n)}_{\mathtt{lin}}(\bm{q}; \theta_n, \rho_n)$, and $V_n(\bm{\Theta}, \bm{\rho})$ are written as $\SINR^{(n)}_{\mathtt{lin}} $, $\RSS^{(n)}_{\mathtt{lin}}$, and $V_n$, respectively, for the brevity of notation.
\end{Proposition}
The proof of Proposition \ref{gradient-vector-theta-SM} is provided in Appendix \ref{Appendix_E}.



Next, we provide the expression for the gradient of $\Phi_{{\sf SM}}$ w.r.t. the BS transmission powers $\bm{\rho}$.

\begin{Proposition}\label{gradient-SM-wrt-power}
    The partial derivative of $\Phi_{{\sf SM}}$ w.r.t. the BS transmission power $\rho_n$ is given by Eq. (\ref{gradient-SM-wrt-power-equation}),
% \begin{multline}\label{gradient-SM-wrt-power-equation}
%     \frac{\partial \Phi_{{\sf SM}}}{\partial \rho_n} = \Bigg\{ \sum_{u=1}^{N_U} \int_{V_n \cap Q_u} \frac{\alpha\xi \cdot \mathtt{SINR_{\mathtt{lin}}^{(n)}} \cdot \frac{\ln(10)}{10} }{\big(\mathtt{SINR_{\mathtt{lin}}^{(n)}} + \nu\big)^{\xi + 1}} \cdot e^{\frac{\alpha}{\big(\mathtt{SINR^{(n)}_{\mathtt{lin}}} + \nu \big)^\xi}}   \lambda(\bm{q})d\bm{q} \\+ \int_{V_n \cap Q_G} \frac{\alpha\xi \cdot \mathtt{SINR_{\mathtt{lin}}^{(n)}} \cdot \frac{\ln(10)}{10} }{\big(\mathtt{SINR_{\mathtt{lin}}^{(n)}} + \nu\big)^{\xi + 1}} \cdot e^{\frac{\alpha}{\big(\mathtt{SINR^{(n)}_{\mathtt{lin}}} + \nu \big)^\xi}}   \lambda(\bm{q})d\bm{q} \Bigg\} \\
%     - \sum_{i\neq n} \Bigg\{ \sum_{u=1}^{N_U} \int_{V_i \cap Q_u} \frac{\alpha\xi\cdot \big[\mathtt{SINR_{\mathtt{lin}}^{(i)}} \big]^2 \cdot \frac{\ln(10)}{10}\cdot \mathtt{RSS_{\mathtt{lin}}^{(n)}}    }{\big(\mathtt{SINR_{\mathtt{lin}}^{(i)}} + \nu\big)^{\xi + 1}\cdot \mathtt{RSS_{\mathtt{lin}}^{(i)}}} \cdot e^{\frac{\alpha}{\big(\mathtt{SINR_{\mathtt{lin}}^{(i)}} + \nu \big)^\xi}}   \lambda(\bm{q})d\bm{q} \\+ \int_{V_i \cap Q_G} \frac{\alpha\xi\cdot \big[\mathtt{SINR_{\mathtt{lin}}^{(i)}} \big]^2 \cdot \frac{\ln(10)}{10}\cdot \mathtt{RSS_{\mathtt{lin}}^{(n)}}    }{\big(\mathtt{SINR_{\mathtt{lin}}^{(i)}} + \nu\big)^{\xi + 1}\cdot \mathtt{RSS_{\mathtt{lin}}^{(i)}}} \cdot e^{\frac{\alpha}{\big(\mathtt{SINR_{\mathtt{lin}}^{(i)}} + \nu \big)^\xi}}   \lambda(\bm{q})d\bm{q}  \Bigg \},
% \end{multline}
where $\SINR^{(n)}_{\mathtt{lin}} $, $\RSS^{(n)}_{\mathtt{lin}}$, and $V_n$ are shorts for $\SINR^{(n)}_{\mathtt{lin}}(\bm{q}; \bm{\Theta}, \bm{\rho})$, $\RSS^{(n)}_{\mathtt{lin}}(\bm{q}; \theta_n, \rho_n)$, and $V_n(\bm{\Theta}, \bm{\rho})$, respectively, for the brevity of notation.
\end{Proposition}
The proof of Proposition \ref{gradient-SM-wrt-power} is provided in Appendix \ref{Appendix_F}.



\subsubsection{Proposed Algorithm}\label{SM-Algorithm}
Using Propositions \ref{optimal-cell-partitioning-SM-performance}, \ref{gradient-vector-theta-SM}, and \ref{gradient-SM-wrt-power}, we design an alternating optimization algorithm, called the soft max-min power allocation and vertical antenna tilt (SMM-PA-VAT) optimization algorithm, similar to Algorithm \ref{BS_PA_VAT_Algorithm}.
%By utilizing Propositions \ref{optimal-cell-partitioning-SM-performance}, \ref{gradient-vector-theta-SM}, and \ref{gradient-SM-wrt-power}, we can create an alternating optimization algorithm that aims to maximize the constrained optimization problem presented in Eqs. (\ref{soft-max-min-objective}) and (\ref{soft-max-min-objective-constraint}) regarding the cell partitioning $\bm{V}$, BS vertical antenna tilts $\bm{\Theta}$, and BS transmission powers $\bm{\rho}$. This algorithm, which we call the soft max-min power allocation and vertical antenna tilt (SMM-PA-VAT) optimization algorithm, closely resembles the iterative process outlined in Algorithm \ref{BS_PA_VAT_Algorithm}, and it can be described as follows: Starting from a random initialization of $\bm{V}$, $\bm{\Theta}$, and $\bm{\rho}$, iteratively: (i) update the cell partitioning $\bm{V}$ according to Proposition \ref{optimal-cell-partitioning-SM-performance} while $\bm{\Theta}$ and $\bm{\rho}$ are fixed; (ii) update the BS vertical antenna tilts $\bm{\Theta}$ using the gradient ascent algorithm, with gradient vector calculated according to Proposition \ref{gradient-vector-theta-SM}, while $\bm{V}$ and $\bm{\rho}$ are fixed; and (iii) update the BS transmission powers $\bm{\rho}$ via the projected gradient ascent algorithm, with the gradient vector computed according to Proposition \ref{gradient-SM-wrt-power}, while $\bm{V}$ and $\bm{\Theta}$ are fixed. The algorithm iterates between these three steps until converged, i.e., when the relative improvement in the performance function $\Phi_{{\sf SM}}$ falls below a predefined threshold. 


\begin{Proposition}\label{SMM-PA-VAT-convergence}
    The SMM-PA-VAT algorithm is an iterative improvement algorithm and converges.
\end{Proposition}
The convergence proof resembles that of Proposition \ref{BS-PA-VAT-convergence} and  is omitted. 