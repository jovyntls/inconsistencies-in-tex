 %!TEX program = pdflatex
 \documentclass[lettersize,journal]{IEEEtran}
\usepackage{amsfonts}
\usepackage{graphicx}
\usepackage{color}
\usepackage{amsmath,amsfonts,amssymb,amsthm,epsfig,epstopdf,url,array}
\usepackage{url,textcomp}
\usepackage{authblk}
\usepackage{cite}
\usepackage{verbatim}
\usepackage{subfigure}
% \usepackage{caption}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{bbm}
\usepackage{amsmath,amsfonts}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{array}
\usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
\usepackage{textcomp}
\usepackage{stfloats}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{cite}
\renewcommand{\algorithmicrequire}{ \textbf{Input:}} %Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{ \textbf{Output:}} %UseOutput in the format of Algorithm
\newcommand{\bs}{\boldsymbol}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{mydef}{Definition}
\newtheorem{remark}{Remark}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\makeatletter
\newcommand{\rmnum}[1]{\romannumeral #1}
\newcommand{\Rmnum}[1]{\expandafter\@slowromancap\romannumeral #1@}
\makeatother
\begin{document}
\title{Deep Reinforcement Learning Empowered Rate Selection of XP-HARQ}
\author{Da~Wu,
        Jiahui~Feng,
        Zheng~Shi,
        Hongjiang Lei,
        Guanghua Yang,
        and Shaodan Ma
%\thanks{\emph{Corresponding Author: Zheng Shi.}}
%\thanks{Da~Wu, Jiahui~Feng, Zheng Shi, and Guanghua Yang are with Jinan University, Zhuhai 519070, China (e-mails: 0x8a@stu2021.jnu.edu.cn; jiahui@stu2020.jnu.edu.cn; zhengshi@jnu.edu.cn; ghyang@jnu.edu.cn).}
%\thanks{H. Lei is with Chongqing University of Posts and Telecommunications, Chongqing 400065, China (e-mail: leihj@cqupt.edu.cn).}
%\thanks{Shaodan Ma is with University of Macau, Macau, China (e-mail: shaodanma@um.edu.mo).}
%\thanks{\emph{Corresponding Author: Zheng Shi.}}
\thanks{This work was supported in part by National Natural Science Foundation of China under Grants 62171200, 62171201, 61971080, and 62261160650, in part by Chongqing Key Laboratory of Mobile Communications Technology under Grant cqupt-mct-202204, in part by Guangdong Basic and Applied Basic Research Foundation under Grant 2023A1515010900, in part by Zhuhai Basic and Applied Basic Research Foundation under Grant ZH22017003210050PWC, in part by the Major Talent Program of Guangdong Provincial under Grant 2019QN01S103, amd in part by the Science and Technology Development Fund, Macau SAR under Grants 0087/2022/AFJ and SKL-IOTSC(UM)-2021-2023. (\emph{Corresponding Author: Zheng Shi.})}
\thanks{Da~Wu, Jiahui~Feng, Zheng Shi, and Guanghua Yang are with the School of Intelligent Systems Science and Engineering, Jinan University, Zhuhai 519070, China (e-mails: 0x8a@stu2021.jnu.edu.cn; jiahui@stu2020.jnu.edu.cn; zhengshi@jnu.edu.cn; ghyang@jnu.edu.cn).}
\thanks{H. Lei is with Chongqing Key Lab of Mobile Communications Technology \& Chongqing University of Posts and Telecommunications, Chongqing 400065, China (e-mail: leihj@cqupt.edu.cn).}
\thanks{Shaodan Ma is the State Key Laboratory of Internet of Things for Smart City, University of Macau, Macau, China (e-mail: shaodanma@um.edu.mo).}
        }
\maketitle
\begin{abstract}
The complex transmission mechanism of cross-packet hybrid automatic repeat request (XP-HARQ) hinders its optimal system design. To overcome this difficulty, this letter attempts to use the deep reinforcement learning (DRL) to solve the rate selection problem of XP-HARQ over correlated fading channels. In particular, the long term average throughput (LTAT) is maximized by properly choosing the incremental information rate for each HARQ round on the basis of the outdated channel state information (CSI) available at the transmitter. The rate selection problem is first converted into a Markov decision process (MDP), which is then solved by capitalizing on the algorithm of deep deterministic policy gradient (DDPG) with prioritized experience replay. The simulation results finally corroborate the superiority of the proposed XP-HARQ scheme over the conventional HARQ with incremental redundancy (HARQ-IR) and the XP-HARQ with only statistical CSI.

   % We consider the rate selection problem using Cross-packet hybrid automatic repeat request (XP-HARQ) in fading channels, where the channel state information (CSI) at the transmitter is only known after a one-slot delay. Since the transmitter has only one delayed CSI, XP-HARQ is used for error recovery. The problem is to determine the transmission rate in each slot so that the time throughput is maximized with the transmit power fixed.
     %We obtain the structure of the optimal policy, and also provide the incremental redundancy HARQ (IR-HARQ) scheme based on PER-DDPG for comparison.
    %The simulation results show that the proposed scheme can effectively improve the throughput. Furthermore, it is also shown that our proposed DRL-based method is the state-of-the-art method for solving the NP-hard rate selection problem, especially when the XP-HARQ maximum number of retransmissions is larger than 3.

   % highlight:
    %correlated channel model!!
\end{abstract}
\begin{IEEEkeywords}
    Cross-packet hybrid automatic repeat request (XP-HARQ), deep reinforcement learning (DRL), outdated channel state information, rate selection.
\end{IEEEkeywords}
\IEEEpeerreviewmaketitle
\hyphenation{HARQ}
\section{Introduction}\label{sec:int}
%With the wide application of 5G technology, scientists have begun to study 6G mobile communication technology. In addition, scientists have given a functional definition of the next generation of communication -- the Internet of Everything, which requires  \cite{9349580,9745424,9332024}
%the 6G system to have higher Capacity, higher data rates, lower latency, higher security and better quality of service (QoS). However,
%To confront the ultra-high reliability and extremely-low latency demands of the next-generation wireless communications (6G), various enabling technologies have been proposed recently. % \cite{vaezi2022cellular}.


Hybrid automatic repeat request (HARQ) is one of the key technologies that is capable of offering reliable transmissions. However, this benefit is essentially reaped at the price of large transmission delay, % \cite{7959548},
which is unfavorable for fulfilling the ultra-reliable and low-latency communications (URLLC). % \cite{sutton2019enabling}.
To resolve such a dilemma, there is a urgent need to develop a flexible HARQ transmission mechanism that could be reconfigurable to meet diverse URLLC requirements. In this letter, we focus on the cross-packet HARQ (XP-HARQ) that is an evolutionary version of HARQ with high spectral efficiency, albeit at the price of high complexity \cite{jabi2017adaptive,jabi2017boost,jabi2018amc}. Unlike the conventional HARQ schemes, new information bits are introduced in retransmissions such that surplus wireless resources are substantially exploited. Hence, it is unnecessary to wait for the end of the retransmissions of the current message before the delivery of the next message especially under benign channel conditions. As a consequence, the spectral efficiency of HARQ is boosted, meanwhile the average transmission delay is reduced.

Recently, the investigations on the XP-HARQ scheme are still in their fancy. Several efforts have been made to accurately evaluate and optimally design XP-HARQ schemes. In \cite{jabi2017adaptive}, Mohammed Jabi {\emph {et al.}} examined the long term average throughput (LTAT) of XP-HARQ, with which the throughput improvement gained by XP-HARQ was verified. In \cite{jabi2017boost}, a two-layer coding scheme was developed to implement XP-HARQ to guarantee the inputs of the encoder with the same length, where puncturing and mixing operations were leveraged. The puncturing rates were then optimized with dynamic programming in \cite{jabi2017boost}. The adaptive modulation and coding scheme was further introduced to boost the LTAT of XP-HARQ in \cite{jabi2018amc}. In \cite{shi2019effective}, the effective capacity of XP-HARQ was analyzed for buffer-limited XP-HARQ. However, the performance metrics of XP-HARQ in \cite{jabi2017adaptive,jabi2017boost,jabi2018amc,shi2019effective} were obtained by conducting Monte-Carlo simulations and lacked insightful analysis. To fill this vacancy, the most fundamental performance metric, namely, outage probability, was derived in closed-form for XP-HARQ over independent Rayleigh fading channels in \cite{9768121}, with which full time diversity of XP-HARQ was proved. However, even under such a simple channel model, the outage analysis is too complex to further assist the optimal design of XP-HARQ, not to mention under more complicated fading channels.
%In recent years, in order to solve the above NP-hard problems, many scientific researches have adopted some model-free, data-driven deep reinforcement learning methods.



%it is hardly possible to extend the complex analytical results in \cite{9768121} to correlated fading channels, which commonly take place in the environment of low-to-medium mobility. More importantly, these complex outage expressions impede the optimal design of XP-HARQ.


%However, the outage probability expression of XP-HARQ obtained in \cite{9768121} is too complicated, and traditional methods cannot provide a dynamic adaptive wireless resource allocation algorithm for time-varying and uncertain complex wireless network environments, so as far as the authors know, XP-HARQ Rate optimization choices have not been addressed in the existing literature.

%In the traditional HARQ field, many scholars have done a lot of research on rate allocation. For example, in \cite{6214030}, the authors studied maximizing the long-term average transmission rate in a two-hop relay network with Chase Combination (CC) type Hybrid Automatic Repeat Request (HARQ), which according to the outage probability, proposes a Numerical search algorithms are used to find the optimal round transmission rate, %however no attention has been paid to cooperative HARQ systems, especially time-correlated channels.

%In \cite{7247293}, the authors study the rate selection problem in a cooperative HARQ-CC system over a time-correlated Nakagami-m fading channel, and derive the outage probability and delay-limited throughput (DLT) in closed form, leading to the maximum Rate selection for DLT.

%And because the HARQ-IR scheme will produce higher spectral efficiency than the HARQ-CC scheme, in \cite{7763302}, the author studies the rate selection method using the incremental redundancy type hybrid automatic repeat request (HARQ-IR) scheme under Rayleigh block fading channel, the author formulates a simplified problem with approximate utility and constraint function by using Gaussian approximation of mutual information and selecting the optimal rate of interruption.

%The experimental results show that the HARQ-IR scheme significantly improves the HARQ-CC scheme. LATR performance, similarly, the authors in \cite{8320562} proposed a rate selection method using the incremental redundancy hybrid automatic repeat request (HARQ-IR) scheme in wireless multicast systems based on \cite{7763302}.

%In \cite{6502167}, the authors used a dynamic programming framework based on non-real-time CSI and optimized the rate allocation strategy for incrementally redundant hybrid ARQ transmissions in Rayleigh channels. However, the related research on XP-HARQ has just started, and there is no related research work on XP-HARQ rate selection at present.


%it is difficult to meet the stringent low latency and high reliability requirements at the same time based on existing technologies. Various enabling technologies have been proposed successively, such as hybrid automatic repeat (HARQ), non-orthogonal multiple access, terminal direct communication, etc. \cite{sutton2019enabling}. One of the most representative supporting technologies is HARQ.
%HARQ is an extremely important cross-layer technology that enables reliable transmission. Its core integrates the retransmission mechanism of the link layer and the forward error correction coding of the physical layer. It is widely used in signaling systems \cite{sutton2019enabling}. %In 5GNR, on-demand HARQ was introduced to increase the bit rate, and Fast HARQ was introduced to speed up ACK feedback.
%However, HARQ essentially gains reliability at the expense of time, which inevitably causes a certain transmission delay \cite{8274949}.

%To solve this problem, it is urgent to explore a new efficient and flexible HARQ transmission mechanism. Different from the traditional HARQ, Cross-packet hybrid automatic repeat request (XP-HARQ) based on the concept of ``multi-packet retransmission'' shows good flexibility and performance advantages in the three dimensions of ``reliability'', ``delay" and ``throughput". It is precisely because of the characteristics of XP-HARQ that how to reasonably allocate the important wireless resource of the transmission rate becomes the key.



 %In recent years, in order to solve the above NP-hard problems, many scientific researches have adopted some model-free, data-driven deep reinforcement learning methods. In \cite{9568962,su2020optimal}, the authors developed a deep Q-network (DQN) composed of RL and deep neural network (DNN) for relay-assisted communication.

 %Inspired by this, this letter incorporate the PER technique into the Deep Deterministic Policy Gradient (DDPG) algorithm and further improve the PER-DDPG learning framework, then we study rate selection in XP-HARQ systems over time-correlated Rayleigh block fading channels question. Compared with existing RL-based studies, simulation results demonstrate that the proposed method can achieve precise control over continuous action spaces and perform effective empirical learning of stable action policies.

To address the above issue, we resort to the data-driven deep reinforcement learning (DRL) for the optimal design of XP-HARQ over correlated fading channels. It should be noticed that only a few works attempted to devise the conventional HARQ schemes using the DRL methods. Particularly, in \cite{9376717}, a DRL enabled user scheduling policy was designed to minimize the age of information (AoI) for HARQ systems. In \cite{9217354}, a deep deterministic policy gradient (DDPG) algorithm was leveraged to maximize the throughput via optimizing the incremental redundancy bits. Unfortunately, the extension of the DRL methods to general HARQ schemes has never been reported. This letter maximizes the LTAT via adaptive rate selection by considering outdated channel state information (CSI). The optimization problem is firstly formulated as a problem of Markov decision process (MDP). By taking into account the continuous state and action spaces, the problem is then solved by using DDPG with prioritized experience replay. By conducting Monte Carlo simulations, the proposed XP-HARQ scheme is proved to be superior to the conventional HARQ with incremental redundancy (HARQ-IR) and the XP-HARQ with only statistical CSI. Furthermore, it is found that the time correlation among fading channels does not lead to a significant impact upon the LTAT of the proposed XP-HARQ scheme.

The rest of this letter is outlined as follows. Section \ref{sec:sys mod} introduces the system model. Section \ref{sec:rein} develops a DRL empowered rate selection algorithm for XP-HARQ.%, where the DRL is implemented with DDPG networks.
The simulated results are presented %for verifications and discussions
in Section \ref{sec:eva}. Section \ref{sec:con} finally concludes this letter.
\begin{figure}[!t]
    \centering
    \includegraphics[width=8cm]{XP-HARQ_model-after.eps}
    \caption{An example of the XP-HARQ scheme with $K=3$.}
    \label{Fig.XP-HARQ model} % ��?����???����y��?��?����??
\end{figure}
%\begin{figure*}[!t]
%    \centering
%    \includegraphics[width=10cm]{XP-HARQ_model-after.pdf}
%    \caption{An example of the XP-HARQ scheme with $K=3$.}
%    \label{Fig.XP-HARQ model} % ��?����???����y��?��?����??
%\end{figure*}

\section{System Model}\label{sec:sys mod}
This letter considers a point-to-point communication system, in which XP-HARQ is adopted to enable the retransmissions of the message. To start, this section delineates the system model, including the XP-HARQ transmission mechanism, the channel model, performance metrics, and the rate selection problem.

\subsection{XP-HARQ}
 %By assuming a time-dependent block-flat Rayleigh fading channel (that is, the channel coefficients remain constant within each block, but vary time-dependently between blocks), for which the sender does not know any prior information about the stochastic process of the channel state.


As shown in Fig. \ref{Fig.XP-HARQ model}, an example is used to illustrate the transmission mechanism of the XP-HARQ. To avoid network congestion in unfavorable propagation environment, the number of transmissions of XP-HARQ is limited up to $K$. For notational simplicity, let $\frak{n}(t) \in \mathbb Z^{+}$ and $\kappa(t) \in [1,K]$ be the functions that map the time slot $t$ to the current HARQ cycle and the current transmission round, respectively. In the initial transmission round of the $\frak{n}(t)$-th HARQ cycle, the message ${\rm m}_{\frak{n}(t),1}$ is encoded as a codeword ${\bf x}_{\frak{n}(t),1}$ with a transmission rate $R_1$. The received signal ${\bf y}_{\frak{n}(t),1}$ reads as
\begin{equation}
    {\bf y}_{\frak{n}(t),1} = \sqrt{P_1} h_{\frak{n}(t),1} {\bf x}_{\frak{n}(t),1} + {\bf n}_{\frak{n}(t),1},
\end{equation}
where $h_{\frak{n}(t),1}$ denotes the channel coefficient of the first round of the $\frak{n}(t)$-th HARQ cycle with ${\mathbb E}(|h_{\frak{n}(t),1}|^2)=1$, ${\bf n}_{\frak{n}(t),1}$ stands for the complex additive Gaussian noise (AWGN) having zero mean and a variance of $\sigma^2$, and $P_1$ is the average transmit power in the initial HARQ round. If ${\bf x}_{\frak{n}(t),1}$ is successfully decoded, a positive acknowledgement (ACK) will be sent back to confirm the successful reception of ${\rm m}_{\frak{n}(t),1}$ and the next HARQ cycle with index $t+1$ will be triggered immediately. Otherwise, a negative acknowledgement (NACK) will be fed back to initiate the retransmissions. According to the coding strategy of XP-HARQ \cite{jabi2017adaptive}, as opposed to the conventional HARQ-IR that only redundant information bits are retransmitted, new information bits are introduced in the retransmissions by XP-HARQ to substantially exploited wireless resources. Accordingly, prior to the $\kappa(t)$-th transmission of the $\frak{n}(t)$-th HARQ cycle, the previously failed messages ${\rm m}_{\frak{n}(t),1},\cdots,{\rm m}_{\frak{n}(t),\kappa(t)-1}$ are combined with the currently received message ${\rm m}_{\frak{n}(t),\kappa(t)}$ to form a longer  message ${{\rm m}_{\frak{n}(t),[\kappa(t)]}}$. The concatenated message ${{\rm m}_{\frak{n}(t),[\kappa(t)]}}$ is encoded as a codeword ${\bf x}_{\frak{n}(t),\kappa(t)}$ with a nominal transmission rate $\sum\nolimits_{\kappa=1}^{\kappa(t)} R_{\kappa}\triangleq R_{\kappa(t)}^{\Sigma}$, where the increment of the transmission rate, i.e., $R_{\kappa}$, originates from the new information bits involved in the $\kappa$-th transmission. Therefore, the signal $\mathbf{y}_{\frak{n}(t),\kappa(t)}$ received in the ${\kappa(t)}$-th round of the current XP-HARQ cycle is written as
\begin{equation}\label{eqn:receive signal 1}
\mathbf{y}_{\frak{n}(t),\kappa(t)}=\sqrt{P_{\kappa(t)}} h_{\frak{n}(t),\kappa(t)} \mathbf{x}_{\frak{n}(t),\kappa(t)}+\mathbf{n}_{\frak{n}(t),\kappa(t)},
\end{equation}
where $h_{\frak{n}(t),\kappa(t)}$, ${\bf n}_{\frak{n}(t),\kappa(t)}$, and $P_{\kappa(t)}$ follow the similar definitions as $h_{\frak{n}(t),1}$, ${\bf n}_{\frak{n}(t),1}$, and $P_1$, respectively, which are omitted here to save space. The messages ${\rm m}_{\frak{n}(t),1},\cdots,{\rm m}_{\frak{n}(t),\kappa(t)}$ are jointly decoded by using the observations ${y_{1}}, \cdots,{y_{\kappa(t)}}$. The current XP-HARQ cycle stops and the next process begins once the receiver succeeds in reconstructing all the previously delivered messages or the maximum number of HARQ transmission attempts ${\kappa(t)}$ is used. Interested readers are referred to \cite{7878518} for more details of the encoding/decoding implementation of XP-HARQ.

%${P}_{k}$ is the average transmit power, $\mathbf{z}_{k}$ is a zero mean complex additive white Gaussian noise (AWGN) with variance $\sigma^{2}$, %$\mathbf{x}_{k}$ is the message ${m}_{[k]}$ is first encoded and then modulated to $\mathbf{x}_{k}$ at the transmission rate ${R}_{l}$, ${h}_{k}$ is the Rayleigh channel of normalized average power coefficient, namely $\mathbb{E}\left\{\left|h_{1}\right|^{2} \right\}=1$.




%If the receiver successfully decodes the message ${m}_{1}$ in the first round of  current XP-HARQ transmission process, it will send an affirmative acknowledgment message (ACK) back to the sender for this XP-HARQ transmission process is completed, and the next XP-HARQ cycle is triggered to transmit the message ${m}_{2}$. Otherwise, the retransmission mechanism is triggered.

%Co-encode previous failed messages with subsequent messages to avoid ``waste'' of mutual information. More specifically, in the $k$th round of current XP-HARQ cycle, the messages ${m_{1}}, \cdots, {m_{k}}$ are combined together to form a longer message ${m_{[k]}}$. At the receiving end, the concatenated the message ${m_{[k]}}$ is decoded based on all previous observations ${y_{1}}, \cdots,{y_{k}}$.


\subsection{Channel Model}
This letter considers time-correlated Rayleigh flat-fading channels, where the channel keeps constant during each codeword transmission slot and changes time-dependently across consecutive transmission slots. We define $t$ as the index of the time slot in the sequel. %, where ${t \in \mathbbm Z_{+}}$, where $\mathbbm Z_{+}$ refers to the set of positive integers.
For notational simplicity, we use the notation $\hbar _{t} $ to represent $ h_{\frak{n}(t),\kappa(t)} $. %$n = \mathfrak{n}(t)$ represents the index of the HARQ cycle.
 As a commonly used time-correlated channel model that takes place in the environment of low-to-medium mobility, $ \hbar_{t} $ is modeled according to a first-order Gauss-Markov process as \cite{5710995}, i.e., % chen2020decentralized
 %, where time is measured in time slots, indexed by ${t \in \mathbbm Z_{+}}$, where $\mathbbm Z_{+} := \left\{ 1, 2, 3, \cdots \right\}$.
%More specifically, in our simulated environment, the channel states of adjacent time slots will change according to the following
%Gaussian Markov block fading autoregressive models , for which the transmitter does not have any prior knowledge.
\begin{equation}\label{eqn:state transition}
    {\hbar}_{t}=\rho {\hbar}_{t-1}+\sqrt{1-\rho^{2}} {w}_t,
\end{equation}

where ${\rho}$ is the correlation coefficient between ${\hbar}_{t}$ and ${\hbar}_{t-1}$, ${w}_t \sim \mathcal{C N}\left({0}, \sigma^{2} \right)$ denotes the channel discrepancy and is independent of ${\hbar_{t-1}}$. In order to account for the impact of channel aging, the outdated channel state ${\hbar}_{t-1}$ is sent back to the transmitter.

\subsection{Performance Metrics}
\subsubsection{Outage Probability}
The outage probability is an essential performance metric for evaluating the system reliability. The outage probability of XP-HARQ is the probability of the event that the accumulated mutual information in each HARQ round is below the transmission rate. More specifically, the outage probability of XP-HARQ after $K$ HARQ rounds is given by \cite{jabi2017adaptive}
\begin{equation}\label{eq:out}
    f_K = \Pr\left(I_1<R_1,I_2 < R_2^\Sigma,\cdots,I_K < R_K^\Sigma\right),
\end{equation}
where ${I_k} = \sum_{l=1}^k{\log _2}(1 + |{\hbar_l}{|^2}{{{P_l}} {/{{\sigma ^2}}}})$ stands for the accumulated mutual information until the $l$-th transmission.


\subsubsection{Long Term Average Throughput}
The long term average throughput (LTAT) is a frequently used performance metric to evaluate the expected throughput of HARQ systems \cite{Caire2001}. %By capitalizing on the theory of renewal reward processes,7180377, 7959548, 7539672
The LTAT of XP-HARQ system is defined as \cite{jabi2017adaptive}
\begin{equation}\label{eqn:LTAT}
    \begin{aligned}
    \eta_K
    &=\lim_{T\to \infty} \frac{\mathcal R(T)}{T} =\frac{\sum_{k=1}^K R_k\left(f_{k-1}-f_K\right)}{1+\sum_{k=1}^{K-1} f_k},
    %=\frac{R_1^{\Sigma}\left(1-f_1\right)+R_2^{\Sigma}\left(f_1-f_2\right)+\ldots+R_K^{\Sigma}\left(f_{K-1}-f_K\right)}{\left(1-f_1\right)+2 \cdot\left(f_1-f_2\right)+\ldots+K \cdot\left(f_{K-1}\right)} \\
    %=\frac{\sum_{k=1}^K R_k\left(f_{k-1}-f_K\right)}{1+\sum_{k=1}^{K-1} f_k},
    \end{aligned}
\end{equation}
where ${\mathcal R(t)}$ refers to the total number of successfully received information bits till time $t$, and the second equality in \eqref{eqn:LTAT} is derived in \cite{Caire2001,jabi2017adaptive} by capitalizing on the renewal theory if only the statistical CSI is available at the transmitter. %$R_K^{\Sigma} = \sum_{l=1}^K R_l$ denote the cumulative rate after $K$th round of current XP-HARQ cycle,
%$f_k=\operatorname{Pr}\left\{\mathrm{NACK}_k\right\} , k \geq 1$ denotes the probability that the receiver fails to decode the message, particularly, $\mathrm{NACK}_k$ is defined as $\mathrm{NACK}_k=\left\{\mathrm{NACK}_{k-1} \wedge\left(I_k^{\Sigma}<R_k^{\Sigma}\right)\right\}$, respectively, %${I_k} = {\log _2}(1 + {\gamma _k})$ represents the mutual information at the $k$-th transmission, ${\gamma _k} = |{h_k}{|^2}{{{P_k}} \mathord{\left/
%{\vphantom {{{P_k}} {{\sigma ^2}}}} \right.
%\kern-\nulldelimiterspace} {{\sigma ^2}}}$ is the received signal-to-noise ratio (SNR) in the $k$-th HARQ round, $R_k$ denotes the incremental transmission rate introduced by $m_k$.
\subsection{Maximization of LTAT}
This paper aims to maximize the LTAT through optimal rate selection if only the aged channel state information (CSI) is available at the transmitter. The optimization problem of the transmission rates can be formulated as
\begin{equation}\label{eq:ltat_max}
 \begin{array}{*{20}{c l}}
{\mathop {\max }\limits_{{R_1}, \cdots ,{R_K}} }& \eta_K\\
{{\rm{s}}{\rm{.t}}{\rm{.}}}&{0 \le {R_k} \le \bar R,\,k\in [1,K]},
\end{array}
\end{equation}
where the transmission rate $\{R_k,\,k\in [1,K]\}$ is upper bounded by $\bar R$ to avoid frequent outages because of the limited resources. %frequent communication interruptions.
However, due to the time correlation among fading channels in \eqref{eqn:state transition} and the involved outage definition in \eqref{eq:out}, it is hardly possible to get the explicit outage expression. Hence, it is unlikely to solve the LTAT maximization problem in \eqref{eq:ltat_max} with the conventional optimization tools. To overcome this difficulty, we recourse to the deep reinforcement learning (DRL) for the optimal solution of the transmission rate.

\section{DRL Empowered Rate Selection}\label{sec:rein}
%In this letter, our goal is to find the resource allocation strategy of XP-HARQ system to maximize the long-term average throughput by adaptively allocating the transmit rate ${R}_{l}$.
Due to the rapid change of time-varying fading channels, it results in a prohibitively high system overhead to acquire the instantaneous CSI. Therefore, we assume that only the outdated and statistical CSIs are available at the transmitter, including the channel state of the previous slot $\hbar _{t-1}$ and the correlation coefficient $\rho$. Moreover, the transmission rate of the current transmission round for XP-HARQ is determined by the transmission status (success or failure), rates, and channel states in the previous transmission rounds. Towards this end, the proposed optimization problem is transformed into a Markov decision process (MDP), which can be solved with DRL methods.
%we model this problem as MDP and solve it using the DRL algorithm.

\subsection{Problem Reformulation and MDP} %(\textcolor{red}{Please remind yourself that copy from other papers is forbidden! Try to understand the paper and the principle, rewrite it with your own words.})
%However, using (\ref{eqn:LTAT}) directly is intractable, since the outage probability expression of XP-HARQ obtained in \cite{9768121} is too complicated, if the outage probability expression is substituted into (\ref{eqn:LTAT}), this makes the problem too complicated to be solved by traditional methods. To circumvent this difficulty, we employ the Monte Carlo integration method to calculate (or estimate) the throughput of the XP-HARQ system.

%Transform to a MDP problem

%We assume that the sender can access to the CSI of the previous time slot, which can be expressed as $h(t) = {h_{t - 1}}$, so the mathematical formulation of the LTAT maximization problem is shown below.
%\begin{equation}\label{eqn:Problem Formulation}
%    \begin{aligned}
%    \boldsymbol{P}_{1}: \max _{R_{t}} & \mathbb{E}\left[\frac{1}{T} \sum_{t=1}^{T} \mathcal{T}_{t, k(t)}\right] \\
%    \text { s.t. } & \mathbf{C}_{1}: 0 \leq R_{t} \leq R_{\text {max }}, \\ & \mathbf{C}_{2}: 1 \leq k(t) \leq K,
 %   \end{aligned}
%\end{equation}
By using the definition of the LTAT and replacing the limit operation with the expectation (the time average converges to the ensemble average for ergodic processes), the original problem \eqref{eq:ltat_max} can be reformulated as
\begin{equation}\label{eq:ltat_max_ref}
\begin{array}{*{20}{c l}}
{\mathop {\max }\limits_{{R{(t)}}} }& \mathbb E \left(\frac{\mathcal R(T)}{T} \right)=\mathbb E \left(\frac{1}{T}{{\sum\nolimits_{t = 1}^{T} {{\mathcal R_{\frak{n}(t),\kappa(t)}}} }}\right)\\
{{\rm{s}}{\rm{.t}}{\rm{.}}}&{0 \le {R{(t)}} \le \bar R},
\end{array}
\end{equation}
where the expectation is taken over the randomness of the channel states, ${R{(t)}}$ is the effective transmission rate for the new information bits in the time slot $t$, %$\frak{n}(t) \in \mathbb Z^{+}$ and $\kappa(t) \in [1,K]$ are the functions of mapping the time slot $t$ to the indices of the current HARQ cycle and transmission round,
${{\mathcal R_{\frak{n}(t),\kappa(t)}}}$ denotes the effective transmission rate for the successfully received information bits after $\kappa(t)$ rounds during the $\frak{n}(t)$-th HARQ cycle. According to the Shannon theory, the successful decoding occurs if and only if the transmission rate is less than the channel capacity. Therefore, ${{\mathcal R_{\frak{n}(t),\kappa(t)}}}$ can be obtained as
%So if the decoding is successful in the $k$th round, the throughput is calculated as shown below:
\begin{equation}\label{eqn:Throughput}
    {{\mathcal R_{\frak{n}(t),\kappa(t)}}} = \left\{ {\begin{array}{*{20}{c}}
        {R^\Sigma_{\kappa(t)} ,}&{\quad I_{\kappa(t)}  \ge R^\Sigma_{\kappa(t)}}\\
        {0,}&{{\rm{else}}}
        \end{array}} \right..
\end{equation}
%where $k(t)$ is the $k$th round transmission of current XP-HARQ cycle, $\sum\nolimits_l^{k(t)} {{I_l}}$ and $\sum\nolimits_l^{k(t)} {{R_l}}$ denote the cumulative mutual information and cumulative rate after $k$th round of current XP-HARQ cycle.
%(\ref{eqn:Problem Formulation}) is non-convex and NP-hard, and it is difficult to obtain a global optimal solution in practice due to high computational complexity and randomly varying channel conditions. Considering the advantages of deep learning in solving dimensionality reduction with high-dimensional input and output, we model

With the problem reformulation of \eqref{eqn:Throughput}, the adaptive rate selection scheme can be modeled as an MDP, which can be solved by leveraging reinforcement learning (RL) method. The MDP essentially comprises four elements, including environment $\mathcal E$, state space $\mathcal S$, action space $\mathcal A$, and reward space $\mathcal R$. More specifically, at each time step $t$, the process is in state $s_t\in \mathcal S$. According to the current state, the agent makes a decision to choose an action $a_t \in \mathcal A$. After taking the action $a_t$, the next state $s_{t+1}$ is observed along with a reward $r_{t}\in \mathcal R$ received from the environment $\mathcal E$. By mapping the optimal rate selection of XP-HARQ as an MDP, the states, actions, and rewards are designed as follows.
%\subsubsection{Environment}
%The environment is a virtual scene, assuming an XP-HARQ cycle with a maximum number of retransmissions ${K}$, corresponding to the communication system established above.
%\subsubsection{Agent} We assume that only the sender is computationally capable and that there is a perfect feedback link between the sender and the receiver, so the sender is used as an agent. The agent predicts the rate of the current slot based on the following three factors from the last slot feedback: rate, channel state and whether the target node is successfully decoded (ACK/NACK).
\subsubsection{State $s_t$}
To capture the channel aging effect, the historical channel state ${h}_{t}$ is considered into the observation of environment. Moreover, the decoding status of XP-HARQ essentially depends on the accumulated mutual information and rate. Accordingly, the state $s_t$ is a vector consisting of the previously accumulated transmission rate and mutual information intended for the $\frak{n}(t)$-th XP-HARQ, and the aged channel state ${h}_{t-1}$, namely %Given a rapidly changing channel state, there is an inevitable delay in obtaining the CSI of time slot ${t}$. Therefore, we consider the historical channel state ${h(t)}$ and transmission rate ${R}_{t, k(t)}$ for each round of HARQ as a comprehensive observation of the environment, i.e.
    %\begin{equation}\label{eqn:state}
    %{s}_{t} \triangleq\left[{R}_{t,1}, \cdots, {R}_{t,K}, I_{\kappa(t-1)}, {h}_{t-%1}\right]
   % \end{equation}
    \begin{equation}\label{eqn:state}
    {s_t} \buildrel \Delta \over = \left\{ {\begin{array}{*{20}{c}}
{\left( {R_{\kappa (t - 1)}^\Sigma ,{I_{\kappa (t - 1)}},{\hbar_{t - 1}}} \right),}&\frak{n}(t-1)=\frak{n}(t)\\
{\left( {0,0,{\hbar_{t - 1}}} \right),}&{\rm else}
\end{array}} \right.,
    \end{equation}
    wherein the accumulated transmission rate and mutual information for the current HARQ cycle are zero if a new HARQ cycle is initiated, i.e., $\frak{n}(t-1)\ne \frak{n}(t)$.
    %where ${h_{t-1}}$ denotes the CSI between the transmitter and receiver at time slot ${t-1}$, ${{R}_{t,1}}$ denotes the rate of the ${1}$th round of current XP-HARQ cycle at time slot ${t}$, and similarly , ${{R}_{t,K}}$ denotes the rate of the ${K}$th round of current XP-HARQ cycle at time slot ${t}$. At time slot ${t}$, if current XP-HARQ cycle is in the first round then ${s}_{t}\triangleq\left[0, \cdots, {R}_{t,K-1}, 0, {h}_{t-1} \right]$.
\subsubsection{Action $a_t$} %In each time slot, the RL agent needs to select the transmit rate. Therefore, our system action can be defined as
The action is defined as the effective transmission rate for the new information bits in the next HARQ round, i.e.,
    \begin{equation}\label{eqn:action}
    {a}_{t} \triangleq {R{(t)}}.
    \end{equation}
     %where $R_{t, k(t)} \in\left[0, R_{\max }\right]$
\subsubsection{Reward $r_t$} %We set the throughput of XP-HARQ as a reward.
    The reward function can be defined as the effective transmission rate of the successfully received information bits for the current HARQ cycle $\frak{n}(t)$, i.e.,
    \begin{equation}\label{eqn:reward}
    {r_t} = r({s_t},{a_t},{s_{t+1}}) \triangleq  {{\mathcal R_{\frak{n}(t),\kappa(t)}}}.
    \end{equation}
    By noticing the continuous space of the states and actions, the MDP problem can be solved with the DRL, which combines the reinforcement learning and deep neural networks to learn the policy. The details are deferred to the next subsection.
%By considering the advantages of deep learning in solving dimensionality reduction with high-dimensional input and output and use deep reinforcement learning combining deep learning and reinforcement learning to solve this problem.
%In practical applications, due to the limitation of wireless resources such as transmission power and channel capacity, the transmission rate cannot be infinite, so we set the maximum rate $R_{\text {max }}$.

\subsection{DRL Empowered Rate Selection }%(\textcolor{red}{seems telling us how a DRL algorithm works not how the rate selection is solved with DRL...you should think about how to integrate DRL into the problem solving.})
%In what follows,
A DRL based rate selection scheme is proposed for the LTAT maximization of the XP-HARQ. By considering the continuous state and action spaces, a deep deterministic policy gradient (DDPG) with prioritized experience replay will be applied to develop the rate selection framework, as shown in Fig. \ref{Fig.DDPG_framework}. This framework consists of four neural networks, i.e., two policy networks (also termed as the actor network, i.e., $\mu(s_t;\boldsymbol{\theta})$ and $\mu(s_{t+1};\boldsymbol{\theta}^-)$) and two evaluation networks (also termed as the critic network, i.e., $Q(s_t,a_t;\boldsymbol{\omega})$ and $Q(s_{t+1},\hat a_{t+1};\boldsymbol{\omega}^-)$), wherein the target-evaluation and target-policy networks are used to calculate the temporal-difference (TD) target to address the overestimation issue, and these neural networks are parameterized by $\boldsymbol{\theta}$, $\boldsymbol{\theta}^-$, $\boldsymbol{\omega}$, and $\boldsymbol{\omega}^-$. In addition, for the stability and fast convergence, a prioritized experience reply memory pool $\mathcal {M}$ is adopted to collect the agent's experience tuple $e_t = (s_i,a_i,r_i,s_{i+1})$ at each time $t$. At each time step, the four neural networks will be updated with a mini-batch of experience samples $\mathcal B_t$ that are drawn from $\mathcal {M}$ according to the priority of the playback experience, that is, $e_t \sim  \mathcal P (\mathcal M) $ for $\forall e_t \in \mathcal B_t$, where $\mathcal P $ is the probability function defined in \eqref{eqn:prob_def}.
%  according to the priority of the playback experience
%When training the neural network, the neural network is updated by sampling a mini-batch experience units from $\mathcal{B}_t$ each time according to the priority of experience.
In what follows, priority experience playback mechanism and the training processes of the four neural networks are described in detail.

% the proposed DRL framework and optimization strategy for designing transmit rate selection for XP-HARQ. Considering the outdated channel state ${\hbar}_{t-1}$, the transmission rate ${R{(t)}}$ and the accumulative mutual information are continuous, we propose a framework based on a deep deterministic policy gradient (DDPG) with prioritized experience replay to solve it, as shown in the Fig. \ref{Fig.DDPG_framework}.
% Our goal is to train the optimal policy network $\mu (s_t;\theta )$, so that the neural network can quickly select the optimal transmission rate ${R{(t)}}$ according to the state $s_t$ at each moment, so as to maximize LTAT.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=3.5in]{DDPG_frame3}
    \caption{The DDPG network for Rate Selection of XP-HARQ.}
    \label{Fig.DDPG_framework} %��?����???����y��?��?����??
\end{figure}

\subsubsection{Prioritized Experience Replay}
In contrast with the uniform random experience replay, the prioritized experience replay is capable of accelerating the learning process and enhancing the training stability \cite{schaul2015prioritized}. According to the prioritized sampling strategy, the sampling probability $p_i$ of the tuple ${e_i = (s_i , a_i , r_i , s_{i+1})}$ is proportional to the absolute value of TD error $\delta_i$, i.e., %\cite{8122622}
\begin{equation}\label{eqn:prob_def}
    p_i  \propto |\delta_i| + \epsilon,
\end{equation}
where $ \epsilon$ is a positive constant to avoid a zero sampling probability, ${\delta _i} = Q({s_i},{a_i};{\boldsymbol{\omega }}) - {r_i} - \gamma Q({s_{i + 1}},{{\hat a}_{i + 1}};{{\boldsymbol{\omega }}^ - })$ denotes the TD error, and $\gamma$ is the discount factor.

%It stores the experience unit ${e_t = (s_t , a_t , r_t , s_{t+1})}$ after each interaction with the environment, and uses the priority $p_t$ as the weight for sampling learning. We use the TD error $\delta_t$ as the priority, and the larger the value of the TD error, the more can be learned from this experience unit.

\subsubsection{Evaluation Network}
The evaluation network aims to approximate the actual state-action function $Q_\pi(s,a)$ with a neural network parameterized by ${\boldsymbol{\omega}}$. The network parameters ${\boldsymbol{\omega}}$ can be updated with the TD algorithm. More specifically, the loss function is defined as the weighted squared TD error averaged over the sampled mini-batch $\mathcal B_t$, i.e.,
% We want to evaluate the network for increasingly accurate value estimation and therefore use a sample of mini-batch experience units to minimize the following loss function.
\begin{equation}\label{eqn:evaLoss}
L({\boldsymbol{\omega}} ) = \frac{1}{{2|\mathcal{B}_t|}}{\sum\limits_{e_i \in \mathcal B_t}{w_i \delta _i^2}},
\end{equation}
where $|\mathcal{B}_t|$ represents the batch size and the importance-sampling weight $w_i$ is used to eliminate the bias introduced by prioritized sampling and ensure the same learning rate of all samples. According to \cite{schaul2015prioritized}, $w_i$ is given by
\begin{equation}\label{eqn:weight_def}
     w_i \propto {\left(|\mathcal{B}_t| p_i\right)^{-\beta}},
\end{equation}
which $\beta \in [0,1]$ is a hyperparameter that controls the extent of the correction. Then, the gradient descent algorithm is leveraged to update the network parameters  $\boldsymbol{\omega }$ as
\begin{equation}\label{eqn:omega_up}
   \boldsymbol{\omega }_{\rm new} \leftarrow  \boldsymbol{\omega }_{\rm now} - \alpha {\nabla _{\boldsymbol{\omega }}}L({\boldsymbol{\omega }_{\rm now}}),
\end{equation}
where ${\nabla _{\boldsymbol{\omega }}}L({\boldsymbol{\omega }}) = \frac{1}{{|{{\cal B}_t}|}}\sum\nolimits_{{e_i} \in {{\cal B}_t}} {{w_i}{\delta _i}{\nabla _{\bf{\boldsymbol{\omega} }}}Q({s_i},{a_i};{\bf{\omega }})} $ refers to the gradient of the loss function with respect to (w.r.t.) $\boldsymbol{\omega }$, and $\alpha$ is the learning rate.
\subsubsection{Policy Network}
%The role of the strategy network is to make the optimal action $a_t$ according to the given state $s_t$. The score of the evaluation network can reflect how good or bad $\boldsymbol{\theta}$ is.
%The goal of training the policy network is to improve the parameter �� so that the evaluation network can give a higher score. We want the expectation of the evaluation network to score as high as possible, so define the expectation of the evaluation network as the objective function, as follows.
The policy network $\mu(s_t;\boldsymbol{\theta})$ aims to learn action policy by mapping the states to the specific actions. Since the action-value function $Q_\pi(s,a)$ can evaluate the score of the current action policy, the performance objective for $\mu(s_t;\boldsymbol{\theta})$ can be defined as \cite{silver2014deterministic}
\begin{equation}\label{eqn:evaLoss}
J({\boldsymbol{\theta }}) = \frac{1}{{|{{\cal B}_t}|}}\sum\limits_{{e_i} \in {{\cal B}_t}} {Q({s_i},\mu ({s_i};{\boldsymbol{\theta }});{\boldsymbol{\omega }_{\rm now}})}.
\end{equation}
To learn the best policy, the parameters of the policy network can be optimized through the maximization of $J({\boldsymbol{\theta }})$. Accordingly, the gradient ascend method is used to update $\boldsymbol{\theta }$, i.e.,
\begin{equation}\label{eqn:theta_up}
   \boldsymbol{\theta }_{\rm new} \leftarrow  \boldsymbol{\theta }_{\rm now} + \upsilon {\nabla _{\boldsymbol{\theta }}}J({\boldsymbol{\theta }_{\rm now}}),
\end{equation}
where $\upsilon $ is the learning rate, and using chain rule yields ${\nabla _{\boldsymbol{\theta }}}J({\boldsymbol{\theta }}) = \frac{1}{{|{{\cal B}_t}|}}\sum\nolimits_{{e_i} \in {\mathcal B_t}} {{\nabla _{\boldsymbol{\theta }}}\mu ({s_i};{\boldsymbol{\theta }}){\nabla _a}Q({s_i},{{\hat a}_i};{{\boldsymbol{\omega }}_{{\rm{now}}}})} $.

%Use the extracted $N_b$ observations $s_i$ each time to calculate the gradient ${\mathbf {g_i}} \buildrel \Delta \over = \frac{{\sum\limits_{i = 1}^{{N_b}} {{\nabla _{\boldsymbol{\theta }}}q({s_i},\mu ({s_i};{\boldsymbol{\theta }});{\boldsymbol{\omega }})} }}{{{N_b}}}$, using gradient ascent to maximize $J(\boldsymbol \theta )$.
%\subsubsection{Target Policy Network}
\subsubsection{Target Evaluation/Policy Networks}
%The role of the objective evaluation network is to alleviate the overestimation caused by using the TD algorithm.
To further improve the stability, the soft update strategy is applied to update the parameters of the target networks, i.e., $\boldsymbol{\omega ^ - }$ and ${\boldsymbol {\theta ^ - }}$. More specifically, with the new parameters $\boldsymbol{\omega }_{\rm new}$ and $\boldsymbol{\theta }_{\rm new}$ given by \eqref{eqn:omega_up} and \eqref{eqn:theta_up}, respectively, the parameters of the two target networks will be updated as
\begin{equation}
    {\boldsymbol {\omega }_{\rm new} ^ -} \leftarrow \tau {\boldsymbol {\omega}_{\rm new}}  + (1 - \tau) {\boldsymbol {\omega }_{\rm now} ^ -},
\end{equation}
\begin{equation}
    {\boldsymbol {\theta }_{\rm new} ^ -} \leftarrow \tau {\boldsymbol {\theta}_{\rm new}}  + (1 - \tau) {\boldsymbol {\theta}_{\rm now} ^ - },
\end{equation}
where the hyperparameter $\tau\ll 1$.
%The initialization of $\boldsymbol{\omega }$ and $\boldsymbol{\omega ^ - }$ is set to the same value.
%Then old parameters will be soft updated periodically following ${\boldsymbol {\omega ^ - }} \leftarrow \tau {\boldsymbol {\omega}}  + (1 - \tau) {\boldsymbol {\omega ^ - }}$, where $\tau$ is a hyperparameter that needs to be set manually.
%The target policy network acts and updates in the same way as the target evaluation network, and the old parameters will be soft updated periodically as following ${\boldsymbol {\theta ^ - }} \leftarrow \tau {\boldsymbol {\theta}}  + (1 - \tau) {\boldsymbol {\theta ^ - }}$.

%\begin{algorithm}[htb]
%\caption{DDPG enabled rate selection algorithm.}
%\label{alg:Framwork}
%\begin{algorithmic}[1] %?a??1 ������???��?DD????��?��y��?
    %\REQUIRE ~~\\ %??������?��?��?2?��y��oInput
   %     The initial the normalized channel correlation coefcient $\rho$, the maximum number of transmissions $K$.
   % \ENSURE ~~\\ %??������?��?3?��oOutput
    %    Optimal ratio allocation strategy $\mu(s;\theta)$.
 %   \FOR {epoch $=1,2,\cdots$}
%        \STATE Initialize the communication environment, get the state $s_1$;
%        \FOR { $t=1,2,\cdots$}
 %           \STATE Let the actor network make predictions and get the action $a_t$;
 %           \STATE Interacting the action $a_t$ with the environment gets a reward $r_t$ according to (\ref{eqn:reward}) and state $s_{t+1}$ according to (\ref{eqn:state});
%            \STATE Sample mini-batch quadruplets $(s_i, a_i, r_i, s_{i+1})$;
%            \STATE Calculate TD error: ${\delta _i} = q_{i} - {{\widehat y}_i}$
%%            \STATE Update critic network, policy network and target network;
%            \STATE Store the quadruplets $(s_t, a_t, r_t, s_{t+1})$ in the prioritized experience replay buffer $\mathcal{B}$.
%        \ENDFOR
 %   \ENDFOR
%\end{algorithmic}
%\end{algorithm}


%
\section{Simulations and Discussions}\label{sec:eva}
In this section, simulated results are presented for verifications and discussions. For illustration, the system parameters are set as $\sigma^{2} = 1$, ${\rho}=0.4$, and ${\bar R}=10 $~bps/Hz unless otherwise specified. Besides, we assume equal power allocation for XP-HARQ, i.e., $P_{1}=\cdots=P_{K}$, and the average transmit signal-to-noise ratio (SNR) is defined as $P_{1} / \sigma^{2}= \cdots= P_{K} / \sigma^{2} \triangleq { \textsc{snr}}$. %We set normalized channel correlation coefficient  and
To deploy the DDPG, both the actor and critic networks consist of one input layer, three hidden layers, and one output layer. The number of the neurons in the three hidden layers are 100, 50, and 30 neurons, respectively. The three hidden layers of both networks use ``{\emph{ReLu}}'' activation functions. The output layer of the actor network invokes ``{\emph{sigmoid}}'' activation function to restrict the transmission rate within $\bar R$, while the critical network does not leverage any activation function in the output layer. %Similarly, the activation functions of the first three layers of the critic network are the same as the number of neurons.
%The output layer does not use an activation function.
Both the actor and critical networks capitalize on the adaptive moment estimation (Adam) optimizer to update the network parameters, and the learning rates are set to $\upsilon = \alpha = 0.001$. Furthermore, we assume that the number of epochs in the training state is 100, the  number of time slots in each epoch is 6000, the size of the prioritized replay buffer is $|\mathcal M|=20000$, the mini-batch size is $|\mathcal B_t|=512$. In addition, we assume that the weight of the soft update $\tau=0.01$, the discount factor $\gamma=0.9$, the extent of the correction $\beta=0.5$, and the noise variance of the behavior policy $\vartheta^2 = 0.2$.

%$\tau _c=6000$ time slots in the train epoch, and we set $epoch = 100$, the size of , mini-batch size $N_b=512$, Furthermore, $\tau=0.01$, $\gamma=0.9$�� $\beta=0.5$.

%\begin{figure}[htbp]
%    \centering
%    \includegraphics[width=10cm]{Figure_1.eps}
%    \caption{Training comparison.}
%    \label{Fig.Comparison} %��?����???����y��?��?����??
%\end{figure}
%Fig. \ref{Fig.Comparison} shows the training situation of the DDPG-based XP-HARQ algorithm and the PER-DDPG algorithm when the transmit power is 5dB and the maximum number of retransmissions is 3. It can be observed that the training speed and stability of PER-DDPG are better than traditional DDPG.

Fig. \ref{Fig.throughputs} depicts the LTAT performance of XP-HARQ versus of the average transmit SNR under different $K$. To exhibit the superiority of the proposed DRL-empowered rate selection scheme, two baseline HARQ schemes are used for comparison, including the conventional HARQ-IR \cite{7959548} %\cite{7180377}
 and the XP-HARQ with only statistical CSI (labeled as ``S-CSI'' in the figure) \cite{9768121}. The results of XP-HARQ with S-CSI can be regarded as the worst performance limit of our proposed scheme. In the meantime, the ergodic capacity is incorporated for benchmarking purpose or as design guidelines. It is shown in Fig. \ref{Fig.throughputs} that the XP-HARQ scheme performs much better than the HARQ-IR scheme. For example, by fixing ${ \textsc{snr}} = 35$~dB and $K=5$, the XP-HARQ scheme achieves a higher LTAT than the HARQ-IR scheme by around 1.65~bps/Hz. It is also seen from Fig. \ref{Fig.throughputs} that the proposed XP-HARQ scheme with outdated CSI surpasses the XP-HARQ scheme with statistical CSI by around 0.15~bps/Hz. Moreover, as the maximum number of transmissions $K$ increases from 3 to 5, a remarkable performance gain can be attained by both XP-HARQ schemes with the outdated CSI and the statistical CSI, whereas the HARQ-IR scheme achieves a negligible LTAT enhancement particularly at high SNR. This advantage of XP-HARQ attributes to new information bits introduced in retransmissions. Moreover, this merit also brings about a reduced transmission delay.%This shows that our solution is effective in reducing latency

%{\color{red}{description for the comparison with jiahui's paper: }}


%It can be observed that when the transmission power is the same, the throughput when the maximum number of retransmissions $K=5$ is significantly greater than that when $K=3$. When the maximum number of retransmissions and the transmission power are the same, the PER-DDPG algorithm has a higher throughput. The throughput is greater than that of the IR-PER-DDPG algorithm.
\begin{figure}[htbp]
    \centering
    \includegraphics[width=7cm]{Throughput_Power_All.eps}
    \caption{The comparison of the LTAT for different HARQ schemes.}
    \label{Fig.throughputs} %��?����???����y��?��?����??
\end{figure}

%

%Fig. \ref{Fig.outages} plots the outage probability of three HARQ schemes versus the transmit SNR. It can be seen from Fig. \ref{Fig.outages} that the increase of $K$ is beneficial to the improvement of the outage probability. Moreover, roughly speaking, the outage performance of HARQ-IR  outperforms that of XP-HARQ, because the higher LTAT of XP-HARQ comes at the price of lower reliability.
%
%%It can be observed that when the transmission power is the same, the interruption probability when the maximum number of retransmissions $K=5$ is significantly smaller than the interruption probability when $K=3$. When the maximum number of retransmissions and the transmission power are the same, the PER-DDPG algorithm has the same performance. The outage probability is greater than that of the IR-PER-DDPG algorithm.
%%
%\begin{figure}[htbp]
%    \centering
%    \includegraphics[width=6.5cm]{Outages.eps}
%    \caption{The comparison of the outage probability for different HARQ schemes.}
%    \label{Fig.outages} %��?����???����y��?��?����??
%\end{figure}
%

Fig. \ref{Fig.Throughput_Rho} investigates the impact of the time correlation coefficient on the LTAT given a fixed ${ \textsc{snr}} = 20$~dB. Overall, it is not beyond our expectation that the time correlation has a detrimental effect on the LTAT. This is because more time diversity gain can be achieved from fading channels with a lower time correlation \cite{7959548}. Nevertheless, it is noteworthy that the superiority of the proposed XP-HARQ schemes essentially stems from utilizing the outdated CSI. Hence, a low channel correlation will result in less similarity of CSIs between two adjacent transmissions, which limits the time diversity gain from retransmissions. Accordingly, it can be seen from Fig. \ref{Fig.Throughput_Rho} that the LTAT curves slightly decrease with $\rho$.

%the average throughput obtained by the PER-DDPG based XP-HARQ algorithm at a power of 35dB under different channel correlation coefficients. It can be observed that in the case of high SNR, when the maximum number of retransmissions $K=5$, the higher the channel correlation coefficient, the lower the throughput.
%

\begin{figure}[htbp]
    \centering
    \includegraphics[width=7cm]{Throughput_Rho.eps}
    \caption{Impact of correlation coefficient $\rho$.}
    \label{Fig.Throughput_Rho} %��?����???����y��?��?����??
\end{figure}

\section{Conclusion}\label{sec:con}
% We study the problem of using XP-HARQ to allocate transmit rates to maximize throughput in Rayleigh fading. We formulate the problem as a MDP, and since the problem is NP-hard and the transmitter does not know the channel state information in advance, we solve the problem using a deep deterministic strategy algorithm in deep reinforcement learning, and use the prioritized experience replay array to speed up the training process. From the above results, it can be proved that XP-HARQ based on PER-DDPG can stably make optimal decisions under different SNR conditions, and can adapt to changes in the environment most quickly.
Due to the lack of simple analytical results of the performance metrics of XP-HARQ, we applied the DRL to properly select the incremental information rate for XP-HARQ over correlated fading channels, without recourse to the traditional optimization tools. More specifically, the maximization of the LTAT was formulated as a problem of MDP, which can be solved by using the algorithm of DDPG with prioritized experience replay. To demonstrate the efficacy of the proposed XP-HARQ scheme, its LTAT performance was compared to the conventional HARQ-IR and the XP-HARQ with only statistical CSI through simulations. It was found that IR-HARQ is more aggressive than XP-HARQ when determining the initial rate. In the meantime, it was also found that the time correlation has a slightly negative impact on the LTAT of the proposed XP-HARQ scheme.


%ddd
\bibliographystyle{IEEEtran}
\bibliography{References}
\end{document}
