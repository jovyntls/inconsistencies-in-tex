\section{Numerical Experiments}
\label{sec:numexp}

In this section, we evaluate the effectiveness of our proposed approach by conducting numerical experiments and comparing its performance with various competing methods \cite{yu2017polynomial, 8849468, 8919859, das2020coded, dasunifiedtreatment, das2023jsait_submitted}. Note that there are several other works specifically developed for sparse matrix computations. Among them, the approach in \cite{wang2018coded} does not provide resilience to maximum number of stragglers for given storage constraints. The approach in \cite{xhemrishi2022distributed} partitions the worker nodes into untrusted and partly trusted cluster, which is not aligned to our assumption. The approach in \cite{ji2022sparse} assigns some jobs to the central node to reduce the probability of rank-deficiency in the decoding, which is also not in line of our assumptions. So, in the numerical experiment section, we do not consider these approaches.

We explore two different distributed systems: the first one consists of $n = 30$ worker nodes with $s = 5$ stragglers and the other consists of $n = 36$ nodes with $s = 8$ stragglers. We focus on a sparse input matrix $\bfA$ sized $40,000 \times 31,500$ and a dense vector $\bfx$ of length $40,000$. We consider two distinct scenarios in which the sparsity of $\bfA$ is $98\%$, and $99\%$, respectively. This implies that randomly selected $98\%$ and $99\%$ entries, respectively, in the matrix $\bfA$ are zero. It is worth noting that there exist numerous practical instances where data matrices demonstrate such (or, even more) levels of sparsity (refer to \cite{sparsematrices} for specific examples). The experiments are carried out on an AWS (Amazon Web Services) cluster, utilizing a {\tt c5.18xlarge} machine as the central node and {\tt t2.small} machines as the worker nodes.

\vspace{0.05 in}

{\bf Worker computation time:} Table \ref{table:worker_comp} presents a comparison among different methods based on the computation time required by worker nodes to complete their respective tasks. In these scenarios, where $k_A = 25$ or $28$, the approaches described in \cite{yu2017polynomial, 8849468, 8919859} allocate linear combinations of $k_A$ submatrices to the worker nodes. Consequently, the original sparsity of matrix $\bfA$ is lost within the encoded submatrices. As a result, the worker nodes experience a significantly increased processing time for their tasks compared to our proposed approach or the methods outlined in \cite{das2020coded, dasunifiedtreatment, das2023jsait_submitted}, which are specifically designed for sparse matrices and involve smaller weights.

To discuss the effectiveness of our approach in more details, we compare the weight of the coding of our approach against the approach in \cite{das2023jsait_submitted}. In the first scenario, when $n = 30$ and $s = 5$, our approach sets the weight $\Bigl\lceil{\frac{(n-s)(s+1)}{n}}\Bigr\rceil = \Bigl\lceil{\frac{25 \times 6}{30}}\Bigr\rceil = 5$, whereas the approach in \cite{das2023jsait_submitted} uses a weight $\min(s+1, k_A) = \min(6, 25) = 6$. Thus, our approach involves around $17\%$ less computational complexity per worker node, which is supported by the results in Table \ref{table:worker_comp}. Similarly, when $n = 36$ and $s = 8$, our proposed approach involves a weight $\Bigl\lceil{\frac{28 \times 9}{36}}\Bigr\rceil = 7$, which is smaller than the corresponding weight, $s + 1 = 9$, used by the approach in \cite{das2023jsait_submitted}.


\begin{table*}[t]
\caption{{\small Comparison of worker computation time and communication delay (matrix transmission time) for matrix-vector multiplication for $n = 30, s = 5$, and $n = 36, s = 8$, when randomly chosen $98\%$ and $99\%$ entries of matrix $\bfA$ are zero.}}
\vspace{-0.1in}
\label{table:worker_comp}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c c c c c c c c c c c c c}
\hline
\toprule
\multirow{3}{*}{Methods} & & \multicolumn{5}{c}{$n = 30$ and $s = 5$} & &  \multicolumn{5}{c}{$n = 36$ and $s = 8$} \\ \cline{3-7} \cline{9-13}
 & & \multicolumn{2}{c}{Comp. Time (in ms)} & &  \multicolumn{2}{c}{Comm. Delay (in s)} & & \multicolumn{2}{c}{Comp. Time (in ms)} & & \multicolumn{2}{c}{Comm. Delay (in s)}\\ \cline{3-4} \cline{6-7} \cline{9-10} \cline{12-13}
& & $99\%$ &  $98\%$ & & $99\%$ & $ 98\%$ & & $99\%$ &  $ 98\%$ &  & $ 99\%$ &  $ 98\%$   \\
 \midrule
Poly. Code  \cite{yu2017polynomial} & & $61.4$ & $62.3$ & & $0.67$ & $1.14$ & & $55.7$ &  $56.3$ & & $0.52$ & $0.95$  \\
Ortho Poly  \cite{8849468}   & & $62.2$ & $61.7$ & & $0.69$ & $1.17$ & & $56.2$ &  $56.4$ & & $0.49$ & $0.91$  \\
RKRP Code \cite{8919859} & & $60.3$ & $61.1$ & & $0.65$ & $1.11$ & & $56.8$ &  $57.4$ & & $0.51$ & $0.93$  \\
SCS Opt. Sch. \cite{das2020coded} & & $24.1$ & $38.3$ & & $0.24$ & $0.37$ & & $28.1$ & $41.3$ & & $0.28$ & $0.42$  \\
Class-based \cite{dasunifiedtreatment} & & $17.3$ & $28.2$ & & $0.20$ & $0.31$ & & $22.1$ & $33.7$ & & $0.24$ & $0.35$  \\
Cyclic Code \cite{das2023jsait_submitted} & & $19.5$ & $33.4$ & & $0.23$ & $0.35$ & & $26.7$ &  $37.6$ & & $0.27$ & $0.39$  \\
{\bf Proposed Scheme} & & $\mathbf{16.7}$ & $\mathbf{27.7}$ & & $\mathbf{0.19}$ & $\mathbf{0.32}$ & & $\mathbf{21.8}$ &  $\mathbf{33.9}$ & & $\mathbf{0.24}$ & $\mathbf{0.34}$  \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vspace{-0.15in}
\end{table*}%

\vspace{0.04 in}

{\bf Communication delay:} Table \ref{table:worker_comp} also illustrates the delay incurred during the transmission of encoded submatrices from the central node to the worker node. The approaches presented in \cite{yu2017polynomial}, \cite{8849468}, and \cite{8919859} employ dense linear combinations of submatrices, resulting in a significant increase in the number of non-zero entries within the encoded submatrices. Consequently, transmitting these large number of non-zero entries leads to a substantial communication delay within the system. In contrast, our proposed scheme mitigates this issue by utilizing encoded submatrices formed through linear combinations of only a limited number of uncoded submatrices which significantly reduces the corresponding communication delay.

For example, consider the scenario when $n = 36, s = 8$ and $\bfA$ is $99\%$ sparse. In this scenario, the approach in \cite{yu2017polynomial} needs to transmit up to $0.01 \times 28 \times \frac{40,000 \times 31,500}{28} = 1.26 \times 10^7$ number of non-zero entries to each node. The corresponding number for the approach in \cite{das2023jsait_submitted, das2023distributedisit} is $0.01 \times (s+1) \times \frac{40,000 \times 31,500}{28} = 4.05 \times 10^6$. On the other hand, the corresponding number for our proposed method is $0.01 \times \Bigl\lceil{\frac{k_A(s+1)}{n}}\Bigr\rceil \times \frac{40,000 \times 31,500}{28} = 3.15 \times 10^6$, which is smaller than the previous ones, and clarifies the reduction of communication delay as mentioned in Table \ref{table:worker_comp}.

\vspace{0.04 in}

{\bf Numerical stability:} 
Next, we assess the numerical stability of distributed systems using different coded matrix computation techniques. We examine the condition numbers of the decoding matrices for various combinations of $n$ workers and $s$ stragglers. By comparing the worst-case condition number ($\kappa_{worst}$) across different methods, we present the $\kappa_{worst}$ values in Table \ref{table:kappa}. The polynomial code approach \cite{yu2017polynomial} involves ill-conditioned Vandermonde matrices and demonstrates significant numerical instability, as evidenced by its notably high value of $\kappa_{worst}$. Our proposed approach, among the numerically stable methods, exhibits smaller $\kappa_{worst}$ value compared to the method in \cite{8849468} where the condition numbers increases exponentially in terms of $s = n - k_A$. Note that the approach in \cite{8919859} provides slightly smaller $\kappa_{worst}$ value than ours; however, as mentioned in Table \ref{table:worker_comp}, the worker computation time and the communication delay are significantly higher in that case, since they assign dense linear combinations to the worker nodes. 

\begin{table}[t]
\caption{\small Comparison among different approaches in terms of worst case condition number $\left(\kappa_{worst} \right)$ 
%among all different choices of $s$ stragglers out of $n$ worker nodes 
and the corresponding required time for $10$ trials to find a good set of random coefficients}
\vspace{-0.1 in}
\label{table:kappa}
\begin{center}
\begin{small}
\begin{sc}
\begin{tabular}{c c c}
\hline
\toprule
\multirow{2}{*}{Methods}  & $\kappa_{worst}$ for  & Req. time for \;\\
  & $n = 30$, $s = 5$ & $10$ trials\, (in s) \\

 \midrule

\; \; Poly. Code  \cite{yu2017polynomial}   & $1.47 \times 10^{13}$ & $0$ \\
\; \; Ortho-Poly\cite{8849468}    & $1.40 \times 10^8$ & $0$ \\
\; \; RKRP Code\cite{8919859}    & $1.76 \times 10^6$ & $81.84$ \\
\; \; SCS Opt. Sch. \cite{das2020coded}   & $4.68 \times 10^7$ & $1138.6$ \\
\; \; Class based \cite{dasunifiedtreatment} & $7.16\times 10^6$ & $1479.3$ \\
\; \; Cyclic Code  \cite{das2023jsait_submitted} & $1.06 \times 10^7$ & $78.38$ \\
\; \; {\textbf{Prop. Scheme}}  & $\mathbf{{8.21 \times 10^6}}$ & $\mathbf{{77.41}}$ \\
\bottomrule
\end{tabular}
\end{sc}
\end{small}
\end{center}
\vspace{-0.2 in}
\end{table}%

\vspace{0.02 in}

{\bf Coefficient determination time:} 
Next, Table \ref{table:kappa} shows a comparative analysis of various methods with respect to the time required for performing 20 trials to obtain a ``good'' set of random coefficients that ensures numerical stability of the system. As explained in Section \ref{sec:trialtime}, the techniques proposed in \cite{das2020coded} and \cite{dasunifiedtreatment} involve partitioning matrix $\bfA$ into $\Delta_A = \textrm{LCM}(n, k_A)$ block-columns. For instance, when $n = 30$ and $s = 5$, $\Delta_A = 150$ is significantly larger than $k_A = 25$, which denotes the partition level in our approach. Consequently, when dealing with higher-sized matrices to determine the condition number, the methods proposed in \cite{das2020coded} and \cite{dasunifiedtreatment} necessitate considerably more time compared to our approach.

%In short, the dense coded approaches \cite{8849468, 8919859} suffer from a huge communication and computation delay in the case of sparse matrices, whereas the approaches in \cite{das2020coded, dasunifiedtreatment} require a significant amount of time to find a ``good'' set of coefficients to make the system numerically stable. On the other hand, our proposed approach serves as a ``sweet spot'' which reduces the communication and computation delays significantly.

\vspace{0.02 in}

{\bf Trade-off between privacy and worker computation time:} 
Next, we compare the trade-off between protection against information leakage and the worker node computation time. Consider a $99\%$ sparse matrix $\bfA$ of size $40,000 \times 31,500$, i.e., $99\%$ entries of $\bfA$ are zero. We assume the nodes to be honest but curious. Now, according to the discussion in Sec. \ref{sec:private}, we add matrix $\bfS$ to the encoded submatrices of $\bfA$. %Since, the probability of any entry of $\bfS$ being non-zero is $\mu$, the assigned submatrix to the worker node will be denser as $\mu$ increases. While, this will enhance the protection of the system (as discussed in Sec. \ref{sec:private}), it will increase the worker node computation time. 
Fig. \ref{privacy_plot} shows the trade-off between the privacy (in terms of $\mu$) and the worker computation time for two different scenarios of $n$ and $s$. The extreme case $\mu = 0$ indicates that the worker node receives only the coded submatrices as outlined by Alg. \ref{Alg:New_matvec}, and in that case, the computation speed is very high. On the other extreme, as clarified in \eqref{eq:sec}, when $\mu = 1$, i.e., dense noise is added to the assigned submatrices, then $\calI\left(\bar{\bfA}_i; \bfA \right) = 0$, which indicates the full protection against information leakage from the honest but curious worker nodes. However, that comes with a sacrifice in the worker node computation speed. In this experiment, we see that the worker computation time is most sensitive at small values of $\mu$, i.e., when less than 20\% non-zero entries are being added. After this point, privacy can be improved with little downside to computational time. Note that the approaches in \cite{das2020coded, dasunifiedtreatment}, while being specifically suited to sparse matrices, do not address the privacy issue.
 
\input{privacy}
